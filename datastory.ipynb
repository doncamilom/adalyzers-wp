{
 "cells": [
  {
   "cell_type": "raw",
   "id": "778df5e2",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Wikispeedia: An exploration into human strategy through path navigation.\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10980b73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wikispeedia: What the hell is this?\n",
    "\n",
    "\n",
    "Since the earliest records, humans have wandered across endless lands and walked thorugh mysterious paths, in an attempt to uncover the unexplored and find better conditions for their tribes. The rise of agriculture and the industrial era has, however, changed things, turning us into bare sedentary animals... until now! \n",
    "\n",
    "![](images/heComputer.gif)\n",
    "\n",
    "[Wikispeedia](https://dlab.epfl.ch/wikispeedia/play/) is an easy and fun game: You are given two Wikipedia articles, and starting from the first article, your goal is to reach the second one exclusively by following links in the articles you encounter, effectively letting you explore your own paths across the vast wikipedia, for free! Your game data is collected by [The EPFL Data Science Lab](https://dlab.epfl.ch/), which helps us and them better understand some aspects of human behaviour ðŸ˜‰.\n",
    "\n",
    "Using this data, we ask ourselves, are there patterns in the way players navigate the network, that make them more successful? That is, are there any optimal strategy that doesn't require extensive knowledge of the network?\n",
    "\n",
    "Players are faced with multiple stimulae during the playing session, and players' decisions as well as their posterior success may be influenced by them. For example, users might navigate the network through `semantic similarity` optimization, or they might just click the links found in `images` or `tables`, etc.\n",
    "\n",
    "Do these particular features have any effect on players' success? Let's see!\n",
    "\n",
    "Whatever\n",
    "---\n",
    "\n",
    "## A look into the collected data...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea43ce4",
   "metadata": {},
   "source": [
    "How are the data distributed among successful and unsuccessful games? The following plots show \n",
    "\n",
    "1. The distributions of optimal lengths of the proposed games.\n",
    "2. The distributions of path lengths obtained by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the shortest_path distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(2,1, \n",
    "                       figsize=(10,8), \n",
    "                       gridspec_kw={\"hspace\":0.4})\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "\n",
    "successful_counts = (successful_df['shortest_path_length']\n",
    "                     .value_counts()\n",
    "                     .reset_index())\n",
    "successful_counts[\"success\"] = \"successful\"\n",
    "\n",
    "unsuccessful_counts = (unsuccessful_df['shortest_path_length']\n",
    "                       .value_counts()\n",
    "                       .reset_index())\n",
    "unsuccessful_counts[\"success\"] = \"unsuccessful\"\n",
    "\n",
    "all_counts = pd.concat([successful_counts, \n",
    "                        unsuccessful_counts], \n",
    "                       axis=0)\n",
    "\n",
    "sns.barplot(data=all_counts, \n",
    "            x=\"index\", \n",
    "            y=\"shortest_path_length\", \n",
    "            hue=\"success\",\n",
    "            ax=ax[0], \n",
    "            palette=[\"#b2df8a\", \"#1f78b4\"])\n",
    "\n",
    "ax[0].set_ylabel('Absolute Counts'); \n",
    "ax[0].set_xlabel('Shortest Path');\n",
    "ax[0].set_title(\"1. Optimal lengths of the proposed games.\")\n",
    "#Count of human paths, stratified by shortest possible path\")\n",
    "\n",
    "ax[0].legend(loc=1)\n",
    "ax[0].set_ylabel('Absolute Counts') \n",
    "ax[0].set_xlabel('Shortest Path')\n",
    "\n",
    "\n",
    "\n",
    "# Next plot: frequency by human length\n",
    "successful_human_path_lengths_3 = (successful_df\n",
    "                                   [successful_df['shortest_path_length'] == 3]\n",
    "                                   ['human_path_length']\n",
    "                                   .value_counts()\n",
    "                                   .reset_index())\n",
    "\n",
    "successful_human_path_lengths_3[\"success\"] = \"successful\"\n",
    "\n",
    "unsuccessful_human_path_lengths_3 = (unsuccessful_df\n",
    "                                     [unsuccessful_df['shortest_path_length'] == 3]\n",
    "                                     ['human_path_length']\n",
    "                                     .value_counts()\n",
    "                                     .reset_index())\n",
    "\n",
    "unsuccessful_human_path_lengths_3[\"success\"] = \"unsuccessful\"\n",
    "\n",
    "all_path_l3 = pd.concat([successful_human_path_lengths_3,\n",
    "                         unsuccessful_human_path_lengths_3])\n",
    "\n",
    "sns.barplot(data=all_path_l3, \n",
    "            x=\"index\", \n",
    "            y=\"human_path_length\", \n",
    "            hue=\"success\", \n",
    "            ax=ax[1],\n",
    "            palette=[\"#b2df8a\", \"#1f78b4\"])\n",
    "\n",
    "ax[1].set_xlim(-0.5,10.5)\n",
    "\n",
    "ax[1].set_ylabel('Absolute Counts')\n",
    "ax[1].set_xlabel('Human path length')\n",
    "ax[1].set_title(\"2. Path lengths obtained by human players, when optimal length is 3.\")\n",
    "# Count of human path lengths, with shortest possible path = 3\");\n",
    "\n",
    "ax[1].legend(loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302f5d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Given a shortest path length of 3, the majority of human path lengths are 3, 4, 5 for both successful and unsuccessful paths. Let's filter the corresponding DataFrames to only keep these path lengths. The matched analysis will be done with these lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe5942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b9605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb943227",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## And now the real deal\n",
    "\n",
    "We wanna test wheather different strategies lead to success in the Wikispeedia game. With this goal in mind, paths can be characterized through local features of the clicked links and articles. The features we extract are:\n",
    "\n",
    "- Relative position of each hyperlink within the current article.\n",
    "- Is the hyperlink next to an image?\n",
    "- Is the hyperlink inside a table?\n",
    "- Similarity between current and target article.\n",
    "- Hyperlink curvature along the path.\n",
    "\n",
    "Once each path has been featurized we perform a matching analysis that lets us reduce the bias in the distributions of successful and unsuccessful populations. For this, we find pairs of `subjects` (paths) in the dataset, that have very similar features, but differ in the one we want to test (the `treatment`). The new distributions will then be much less biased and we will be able to compare the effect of the treatment variable. \n",
    "\n",
    "### Confounding variables\n",
    "\n",
    "Many factors can affect the analysis, among them:\n",
    "\n",
    "1. the difficulty of each task, and \n",
    "2. the strategy adopted to solve it. \n",
    "\n",
    "To address the first, we naively quantify `difficulty` of a task as the minimum distance between the source and the target, as determined with the Floyed-Warshall algorithm, while the second is addressed through path length, e.g. the total number of links clicked by each player in the given game.\n",
    "\n",
    "Controlling by such variables allows a first approximation to controlling these confounding effects.\n",
    "\n",
    "### Treatment\n",
    "\n",
    "We will test the effect of clicking mostly links in image boxes.\n",
    "\n",
    "The question is then: given an equally difficult task assigned, and having a fixed number of possible choices to perform, are players more successful if they adopt `clicking mainly links in images` as a strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257b8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c78c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef59aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "successful_df = successful_df[(successful_df['shortest_path_length'].apply(lambda x: x == 3))]\n",
    "successful_df = successful_df[(successful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\n",
    "successful_df.head(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unsuccessful_df = unsuccessful_df[(unsuccessful_df['shortest_path_length'].apply(lambda x: x == 3))]\n",
    "unsuccessful_df = unsuccessful_df[(unsuccessful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\n",
    "unsuccessful_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98381d7f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "At this point, we have filtered the successful and unsuccessful DataFrames by the following conditions:\n",
    "\n",
    "    Shortest path length = 3 --> we hypothesize that all paths have the same \"difficulty\" by enforcing this\n",
    "    The actual human path lengths are 3,4,5 --> the majority of human paths are within these lengths\n",
    "\n",
    "The next step is to perform matching of datasets. To make this process possible, we first augment both successful and unsuccessful DataFrames with is_successful Boolean condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8df0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "successful_df['is_successful'] = 1\n",
    "unsuccessful_df['is_successful'] = 0\n",
    "\n",
    "# merge the successful and unsuccessful DataFrames\n",
    "final_df = pd.concat([successful_df, unsuccessful_df])\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "final_df.head(3)\n",
    "\n",
    "# save the DataFrame\n",
    "final_df.to_csv('final_df.csv')\n",
    "\n",
    "# next, we will calculate \"treatments\" involving semantic distance metrics for the DataFrame\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840991a6",
   "metadata": {},
   "source": [
    "\n",
    "2 . Document Similarity\n",
    "Similarity Between Wikipedia Articles: 'Bag-of-Words' Cosine Similarity Based on Word Frequencies\n",
    "\n",
    "We are interested in measuring *similarity* between Wikipedia articles to investigate players' strategies in Wikispeedia. The proposed similarity is Cosine Similarity measured on word frequencies given a pair of articles. Therefore, the assumption we make is that articles with similar occurrences of words are similar.\n",
    "\n",
    "In order to mitigate similarity due to common English words such as \"the\", we apply a filter as follows:\n",
    "\n",
    "    Remove all Stop words as defined in Scikit-learn\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/feature_extraction.html#nqy18\n",
    "\n",
    "    Remove all white spaces and \\n characters\n",
    "\n",
    "    Apply a scaled version of word frequency count as implemented in TfidfTransformer in Scikit-learn:\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\n",
    "\n",
    "The cell blocks below show our data processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "base_path = '.data/plaintext_articles'\n",
    "articles = os.listdir(base_path)\n",
    "# sort the article names in alphabetical order. This is not strictly required\n",
    "articles = sorted(articles)\n",
    "\n",
    "def remove_duplicates_and_fix_names(articles: list):\n",
    "    \"\"\"this function parses all the articles provided in the plaintext folder and stores the\n",
    "       names of all the articles after \"cleaning\" them (some articles containing accents have encoded names).\n",
    "       The raw text from the files are also stored in a list after removing white spaces and empty lines.\"\"\"\n",
    "    \n",
    "    def parse_text(article: str) -> str:\n",
    "        \"\"\"this helper function reads a raw text file and removes white spaces and empty lines.\"\"\"\n",
    "        with open(os.path.join(base_path, article)) as f:\n",
    "            # remove white spaces\n",
    "            raw_text = [line.rstrip() for line in f.readlines()]\n",
    "            # remove empty lines\n",
    "            raw_text = [line.strip() for line in raw_text if line != '']\n",
    "\n",
    "            return str(raw_text)\n",
    "    \n",
    "    # some article names have errors - fix these\n",
    "    article_names_cleaned, texts = [], []\n",
    "    \n",
    "    for article in articles:\n",
    "        # the \"%\" character indicates an encoded name\n",
    "        if \"%\" not in article:\n",
    "            # store the article name\n",
    "            article_names_cleaned.append(article)\n",
    "            # store the raw text from the article\n",
    "            texts.append(parse_text(article))\n",
    "            \n",
    "        else:\n",
    "            with open(os.path.join(base_path, article), 'r') as f:\n",
    "                # extract the correct name. It is always on the 3rd line of the article\n",
    "                correct_name = f.readlines()[2].replace('\\n', '')\n",
    "                # store the article name\n",
    "                article_names_cleaned.append(correct_name)\n",
    "                # store the raw text from the article\n",
    "                texts.append(parse_text(article))\n",
    "                    \n",
    "    \n",
    "    # remove \".txt\" from the article names\n",
    "    article_names_cleaned = [article.replace('.txt', '') for article in article_names_cleaned]\n",
    "    # remove list bracket from string casting of raw text\n",
    "    texts = [text.replace('[', '') for text in texts]\n",
    "    \n",
    "    return article_names_cleaned, texts\n",
    "\n",
    "article_names_cleaned, texts = remove_duplicates_and_fix_names(articles)\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6eb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# count word frequency using sklearn out-of-the-box functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# remove common stop words\n",
    "tfid_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# apply CountVectorizer and TfidfTransform sequentially\n",
    "matrix = tfid_vectorizer.fit_transform(texts)\n",
    "matrix = matrix.todense()\n",
    "\n",
    "# by default, \"TfidfVectorizer\" uses l2 norm and thus, to obtain the Cosine Similarity,\n",
    "# we simply perform a dot product of the matrix\n",
    "cosine_sim = pd.DataFrame(np.dot(np.array(matrix), np.array(matrix).T), columns=article_names_cleaned)\n",
    "# assign the index names also as the article names. This is used for easy querying pairs of articles\n",
    "cosine_sim.index = article_names_cleaned        \n",
    "cosine_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cf98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save the Cosine Similarity matrix so we do not have to re-compute it every time\n",
    "np.save('cosine_similarity.npz', np.array(cosine_sim))\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae7a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's take a look at the Cosine Similarity distribution of the most similar article to a given article\n",
    "most_similar = []\n",
    "for idx in range(len(cosine_sim)):\n",
    "    # index [-2] because index [-1] is always = 1 since it is a self similarity\n",
    "    most_similar.append(sorted(cosine_sim.iloc[idx])[-2])\n",
    "    \n",
    "# plot the similarities\n",
    "pd.Series(most_similar).plot(kind='hist', edgecolor='k')\n",
    "plt.title(\"Cosine Similarity Distribution of Most Similar Articles\")\n",
    "plt.xlabel(\"Cosine Similarity\"); plt.ylabel(\"Absolute Counts\")\n",
    "\n",
    "# there are no glaring red flags, e.g., most Cosine Similarities close to 0 or 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# show a few concrete examples and see if our metric makes empirical sense\n",
    "cosine_sim['Ukraine'].sort_values()\n",
    "\n",
    "# results look reasonable\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59299fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim['Zinc'].sort_values()\n",
    "# results also look reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89867526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# \"Cygni\" are star systems/extrasolar planets and thus are expected to be similar\n",
    "# the time periods have some similarity\n",
    "sns.clustermap(cosine_sim.iloc[10:20, 10:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \"Cancri\" are exoplanets and thus are expected to be similar\n",
    "# we again see similarity between time periods\n",
    "sns.clustermap(cosine_sim.iloc[50:60, 50:60])\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c19371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# after confirming that the Cosine Similarity method is informative, we next show the function\n",
    "# we will use to extract similarity of articles in a navigation path\n",
    "\n",
    "# first show a hard-coded navigation path for illustration\n",
    "navigation_path = '14th_century;Time;Light;Rainbow'\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_path_cosine_similarity(path: str, matrix: pd.DataFrame, method: str='sequential') -> list:\n",
    "    \"\"\"this function takes a navigation path string and returns an array of the Cosine Similarity.\n",
    "       Two methods are supported: 'sequential' (default) which measures the sequential similarity and \n",
    "       'target' which measures the similarity between the current article at the target article.\n",
    "       \n",
    "       Input:\n",
    "       path: navigation path string\n",
    "       matrix: pre-computed Cosine Similarity matrix\n",
    "       method: measurement method (defaults to 'sequential')\n",
    "       \n",
    "       Return:\n",
    "       list: list of similarity measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # there is 1 article that is duplicated in the plaintext folder. Problematically, the duplicated\n",
    "    # article has different names and navigation paths can point to these different names, yet the \n",
    "    # article is the same. This one exception is handled here. In the implementation, we do not have to \n",
    "    # worry about this particular article pointing back to itself as it is not possible\n",
    "    \n",
    "    # navigation paths are separated by \";\"\n",
    "    path = path.split(';')\n",
    "    \n",
    "    # if the path only contains the starting article, return None\n",
    "    if len(path) == 1:\n",
    "        return None\n",
    "    \n",
    "    path_sims = []\n",
    "    \n",
    "    def get_correct_name(query: str, base_path: str='.data/plaintext_articles') -> str:\n",
    "        \"\"\"this nested function is called in case a path contains error characters.\n",
    "           Returns the fixed path so it can be used to query the Cosine Similarity matrix.\"\"\"\n",
    "        if '_' in query and '%' not in query:\n",
    "            return query\n",
    "        elif query in ['Podcasting', 'Color', 'Fencing', 'Anemia', 'Quito']:\n",
    "            return query\n",
    "        else:\n",
    "            with open(os.path.join(base_path, f'{query}.txt'), 'r') as f:\n",
    "                out = f.readlines()[2].replace('\\n', '')\n",
    "                return out\n",
    "    \n",
    "    # compute the similarity between sequential nodes\n",
    "    # output list length is N-1\n",
    "    if method == 'sequential':\n",
    "        for idx in range(0, len(path)-1, 1):\n",
    "            try:\n",
    "                path_sims.append(matrix.loc[path[idx]].loc[path[idx+1]])\n",
    "            except Exception:\n",
    "                # either path or both paths could be wrong - just \"fix\" both\n",
    "                # treat the 1 exception due to duplicated file\n",
    "                if get_correct_name(path[idx]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n",
    "                    sim = float(cosine_sim.loc['Polishâ€“Muscovite War (1605â€“1618)'].drop_duplicates()[get_correct_name(path[idx+1])])\n",
    "                    path_sims.append(sim)\n",
    "                    \n",
    "                elif get_correct_name(path[idx+1]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n",
    "                    sim = cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polishâ€“Muscovite War (1605â€“1618)']\n",
    "                    path_sims.append(sim)\n",
    "                    \n",
    "                else:\n",
    "                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[idx+1])])\n",
    "\n",
    "                    \n",
    "    # compute the similarity between the current node and the target node\n",
    "    # output list length is N\n",
    "    elif method == 'target':\n",
    "        for idx in range(0, len(path), 1):\n",
    "            try:\n",
    "                path_sims.append(matrix.loc[path[idx]].loc[path[-1]])\n",
    "            except Exception:\n",
    "                # either path or both paths could be wrong - just \"fix\" both\n",
    "                # treat the 1 exception due to duplicated file\n",
    "                if get_correct_name(path[idx]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n",
    "                    sim = cosine_sim.loc['Polishâ€“Muscovite War (1605â€“1618)'].drop_duplicates()[get_correct_name(path[-1])]\n",
    "                    path_sims.append(sim)\n",
    "                    \n",
    "                elif get_correct_name(path[-1]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n",
    "                    sim = float(cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polishâ€“Muscovite War (1605â€“1618)'])\n",
    "                    path_sims.append(sim)\n",
    "                    \n",
    "                else:\n",
    "                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[-1])])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Unsupported similarity method: choose from \"sequential\" or \"target\".')\n",
    "        \n",
    "    return path_sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(navigation_path.split(';'))\n",
    "\n",
    "# show the \"sequential\" method\n",
    "sequential_sims = get_path_cosine_similarity(path=navigation_path,\n",
    "                                             matrix=cosine_sim,\n",
    "                                             method='sequential')\n",
    "# intepreted as:\n",
    "# sim(14th_century, Time) = 0.031\n",
    "# sim(Time, Light) = 0.147\n",
    "# sim(Light, Rainbow) = 0.158\n",
    "# the similarity list is always N-1 in length, N = # nodes\n",
    "sequential_sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141cfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# show the \"target\" method\n",
    "target_sims = get_path_cosine_similarity(path=navigation_path,\n",
    "                                         matrix=cosine_sim,\n",
    "                                         method='target')\n",
    "# intepreted as:\n",
    "# sim(14th_century, Rainbow) = 0.008\n",
    "# sim(Time, Rainbow) = 0.035\n",
    "# sim(Light, Rainbow) = 0.158\n",
    "# sim(Rainbow, Rainbow) = 1 (the path was successful so similarity is 1)\n",
    "# the similarity list is now N in length, N = # nodes\n",
    "\n",
    "# note: paths where the last index != 1 means the path was unsuccessful\n",
    "target_sims\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe133ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the \"sequential\" and \"target\" cosine similarities and augment the DataFrame with these metrics\n",
    "final_df['path_seq_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim))\n",
    "final_df['path_target_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim, method='target'))\n",
    "\n",
    "final_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# next, write a function that returns whether the Cosine similarities in a navigation path are increasing/decreasing\n",
    "def sims_to_bool_path(path: str):\n",
    "    \"\"\"this function takes a navgiation path as input and returns the path with same dimensions\n",
    "       containing Booleans denoting whether the Cosine similarity is increasing\"\"\"\n",
    "    bool_path = []\n",
    "    for idx in range(len(path)-1):\n",
    "        if path[idx+1] > path[idx]:\n",
    "            bool_path.append(True)\n",
    "        else:\n",
    "            bool_path.append(False)\n",
    "    \n",
    "    return bool_path\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb520b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_df['path_seq_boolean'] = final_df['path_seq_cosine_sim'].apply(sims_to_bool_path)\n",
    "final_df['path_target_boolean'] = final_df['path_target_cosine_sim'].apply(sims_to_bool_path)\n",
    "\n",
    "final_df.head(3)\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function filters paths that only contain strictly increasing Cosine similarities\n",
    "def filter_strictly_increasing_sims(path_list: list):\n",
    "    return False if False in path_list else True\n",
    "\n",
    "final_df['seq_strictly_increasing'] = final_df['path_seq_boolean'].apply(filter_strictly_increasing_sims)\n",
    "final_df['target_strictly_increasing'] = final_df['path_target_boolean'].apply(filter_strictly_increasing_sims)\n",
    "\n",
    "final_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# the matching algorithm requires some computation time\n",
    "# compute the matched DataFrames all at once and save them so they can be read into memory directly\n",
    "\n",
    "# there are 2 treatments\n",
    "#    1. strictly increasing *sequential* Cosine similarity\n",
    "#    2. strictly increasing *target* Cosine similarity\n",
    "\n",
    "# there are also 3 conditions we filter on\n",
    "#    1. human_path_length = 3\n",
    "#    2. human_path_length = 4\n",
    "#    3. human_path_length = 5\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "for treatment in ['seq_strictly_increasing', 'target_strictly_increasing']:\n",
    "    for length in [3,4,5]:\n",
    "    \n",
    "        treatment_df = final_df[final_df[treatment] == True]\n",
    "        control_df = final_df[final_df[treatment] == False]\n",
    "\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for control_id, control_row in control_df.iterrows():\n",
    "            for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "                # Adds an edge only for the same human path length\n",
    "                if (control_row['human_path_length'] == length and treatment_row['human_path_length'] == length):\n",
    "                    G.add_edge(control_id, treatment_id)\n",
    "\n",
    "        matching = nx.max_weight_matching(G)\n",
    "        matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "        balanced_df = final_df.iloc[matched]\n",
    "\n",
    "        # save balanced_df\n",
    "        balanced_df.to_csv(f'{treatment}_{length}.csv')\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# next, perform logistic regression on each treatment with fixed human path length\n",
    "\n",
    "# read all the matched DataFrames into memory\n",
    "df_seq_3 = pd.read_csv('seq_strictly_increasing_3.csv')\n",
    "df_seq_4 = pd.read_csv('seq_strictly_increasing_4.csv')\n",
    "df_seq_5 = pd.read_csv('seq_strictly_increasing_5.csv')\n",
    "\n",
    "df_target_3 = pd.read_csv('target_strictly_increasing_3.csv')\n",
    "df_target_4 = pd.read_csv('target_strictly_increasing_4.csv')\n",
    "df_target_5 = pd.read_csv('target_strictly_increasing_5.csv')\n",
    "\n",
    "# store all results in a dictionary\n",
    "results = {}\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "for df, experiment_name in zip([df_seq_3, df_seq_4, df_seq_5, df_target_3, df_target_4, df_target_5],\n",
    "                               ['Sequential 3', 'Sequential 4', 'Sequential 5', 'Target 3', 'Target 4', 'Target 5']):\n",
    "    \n",
    "    if 'Sequential' in experiment_name:\n",
    "        mod = smf.logit(formula='is_successful ~  seq_strictly_increasing', data=df)\n",
    "    else:\n",
    "        mod = smf.logit(formula='is_successful ~  target_strictly_increasing', data=df)\n",
    "        \n",
    "    res = mod.fit()\n",
    "    \n",
    "    # dictionary to store the coefficients and p-values of the current experiment\n",
    "    curr_analysis = {}\n",
    "    # store the intercept\n",
    "    curr_analysis['Intercept'] = res.params[0]\n",
    "    # store the intercept p-value\n",
    "    curr_analysis['Intercept p-value'] = res.pvalues[0]\n",
    "    # store the coefficient for **strictly increasing Cosine similarity**\n",
    "    curr_analysis['Coefficient'] = res.params[1]\n",
    "    # store the coefficient p-value\n",
    "    curr_analysis['Coefficient p-value'] = res.pvalues[1]\n",
    "    \n",
    "    results[experiment_name] = curr_analysis\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c599647",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The syntax in the above table is interpreted as :\n",
    "\n",
    "    Sequential means the strictly increasing Cosine similarity was measured for sequential articles\n",
    "    Target means the strictly increasing Cosine similarity was measured based on sequential article compared to the target article\n",
    "    The number indicates the human path length fixed. Previously, we also fixed that the shortest path length is 3.\n",
    "\n",
    "Our research question is:\n",
    "\n",
    "*When given X number of choices in the same difficulty game, how did successful and unsuccessful players differ in their clicking behaviour?*\n",
    "\n",
    "Based on the above table, the p-values for both the incercept and coefficient for logistic regression are significant for all experiments except 1 (Sequential 5). The interpretation for all experiment findings are similar. Concretely, consider the experiment Sequential 3. The interpretation is:\n",
    "\n",
    "    *Given a shortest path length of 3 (fixed difficulty) and given a human path length of 3 (when the human makes 3 choices on article clicks), clicking articles with increasing Cosine similarity between sequential articles (as a proxy to measure semantic meaning), leads to more successful outcomes.*\n",
    "\n",
    "For Target experiments have a similar interpretation. Concretely, consider the experiment Target 5:\n",
    "\n",
    "    *Given a shortest path length of 3 (fixed difficulty) and given a human path length of 5 (when the human makes 5 choices on article clicks), clicking articles with increasing Cosine similarity between the current article and the target article (as a proxy to measure semantic meaning), leads to more successful outcomes.*\n",
    "\n",
    "These findings are interesting and suggest that (potentially contrarily to common sense) clicking articles based on semantic meaning can lead to more successful outcomes in the Wikispeedia game (at least the current version we are investigating).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896402fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f784e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945f725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90552363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-polar\n",
    "#| fig-cap: \"A line plot on a polar axis\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r = np.arange(0, 2, 0.01)\n",
    "theta = 2 * np.pi * r\n",
    "fig, ax = plt.subplots(\n",
    "  subplot_kw = {'projection': 'polar'} \n",
    ")\n",
    "ax.plot(theta, r)\n",
    "ax.set_rticks([0.5, 1, 1.5, 2])\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518677a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
