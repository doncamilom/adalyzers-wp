[
  {
    "objectID": "datastory.html#wikispeedia-what-the-hell-is-this",
    "href": "datastory.html#wikispeedia-what-the-hell-is-this",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Wikispeedia: What the hell is this?",
    "text": "Wikispeedia: What the hell is this?\nSince the earliest records, humans have wandered across endless lands and walked thorugh mysterious paths, in an attempt to uncover the unexplored and find better conditions for their tribes. The rise of agriculture and the industrial era has, however, changed things, turning us into bare sedentary animalsâ€¦ until now!\n\nWikispeedia is an easy and fun game: You are given two Wikipedia articles, and starting from the first article, your goal is to reach the second one exclusively by following links in the articles you encounter, effectively letting you explore your own paths across the vast wikipedia, for free! Your game data is collected by The EPFL Data Science Lab, which helps us and them better understand some aspects of human behaviour ðŸ˜‰.\nUsing this data, we ask ourselves, are there patterns in the way players navigate the network, that make them more successful? That is, are there any optimal strategy that doesnâ€™t require extensive knowledge of the network?\nPlayers are faced with multiple stimulae during the playing session, and playersâ€™ decisions as well as their posterior success may be influenced by them. For example, users might navigate the network through semantic similarity optimization, or they might just click the links found in images or tables, etc.\nDo these particular features have any effect on playersâ€™ success? Letâ€™s see!\nWhatever"
  },
  {
    "objectID": "datastory.html#a-look-into-the-collected-data",
    "href": "datastory.html#a-look-into-the-collected-data",
    "title": "Marilyn Monroe and Relativity theory? Thereâ€™s always a connection! But howâ€¦?",
    "section": "A look into the collected dataâ€¦",
    "text": "A look into the collected dataâ€¦\nWhat data do we have though? How hard are the tasks our brave users had to overcome? Letâ€™s quickly explore this.\n\n\nCode\n# take a look at the shortest_path distributions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nfinal_df = pd.read_csv('processed/final_df_curv_feats.csv')\n\n\nfig, ax = plt.subplots(2,1, \n                       figsize=(10,8), \n                       gridspec_kw={\"hspace\":0.3})\nplt.rcParams[\"font.size\"] = 12\n\n\nsns.barplot(data=(final_df\n                  .groupby([\"shortest_path_length\", \n                            \"is_successful\"])['path']\n                  .count()\n                  .reset_index()\n                 ),\n            y=\"path\", \n            x=\"shortest_path_length\", \n            hue=\"is_successful\",\n            ax=ax[0], \n            palette=[\"#b2df8a\", \"#1f78b4\"])\n\n\nax[0].set_ylabel('Absolute Counts'); \nax[0].set_xlabel('Shortest Path');\nax[0].set_title(\"1. Optimal lengths of the proposed games.\")\n#Count of human paths, stratified by shortest possible path\")\n\nax[0].legend(loc=1)\nax[0].set_ylabel('Absolute Counts') \nax[0].set_xlabel('Shortest Path')\n\n\nsns.barplot(data=(final_df.query(\"shortest_path_length==4\")\n                  .groupby([\"human_path_length\", \n                            \"is_successful\"])['path']\n                  .count()\n                  .reset_index()\n                 ),\n            y=\"path\", \n            x=\"human_path_length\", \n            hue=\"is_successful\",\n            ax=ax[1], \n            palette=[\"#b2df8a\", \"#1f78b4\"])\n\nax[1].set_xlim(-0.5,10.5)\n\nax[1].set_ylabel('Absolute Counts')\nax[1].set_xlabel('Human path length')\nax[1].set_title(\"2. Path lengths obtained by human players, when optimal length is 4.\")\n# Count of human path lengths, with shortest possible path = 3\");\n\nax[1].legend(loc=1);\n\n\n\n\n\nHow are the data distributed among successful and unsuccessful games? We show: (top) The distributions of optimal lengths of the proposed games, and (bottom) The distributions of path lengths obtained by humans.\n\n\n\n\nThe graphs reveal some interesting features of the Wikispeedia hypertextual graph and the humans navigability network. The first plot shows the distribution of the shortest path distance of the tasks assigned to players.\nThe second plot shows the length of the actual paths taken by humans, on the subset with shortest path distance equal to 4. Given that the shortest path distance is taken as a proxy to game difficulty (see Section A matching study): - the distribution of the difficulty of games peaks at length equal to 3 and ranges from 2 to 5; - there exist hyperlinks which are not connected to any other; - as difficulty increases, the class unbalance decreases; - the distribution of successful human paths (with fixed shortest path distance == 4) spreads to a wide range of values, non monotously. - the distribution of unsuccessful paths (with fixed shortest path distance == 4) is severely skewed to the left and peaks at 3, meaning that in such paths, players give up before even having a chance to win.\nMore down-to-Earth: the distribution of optimal path lengths and the distribution of human path lengths are uneven and significantly vary for the populations of successful vs unsuccessful players.  These conclusions bring us to look for features of the games possibly affecting players performances (a.k.a. covariates), leading them to behave so inhomogeneously."
  },
  {
    "objectID": "datastory.html#and-now-the-real-deal",
    "href": "datastory.html#and-now-the-real-deal",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "And now the real deal",
    "text": "And now the real deal\nWe wanna test wheather different strategies lead to success in the Wikispeedia game. With this goal in mind, paths can be characterized through local features of the clicked links and articles. The features we extract are:\n\nRelative position of each hyperlink within the current article.\nIs the hyperlink next to an image?\nIs the hyperlink inside a table?\nSimilarity between current and target article.\nHyperlink curvature along the path.\n\nOnce each path has been featurized we perform a matching analysis that lets us reduce the bias in the distributions of successful and unsuccessful populations. For this, we find pairs of subjects (paths) in the dataset, that have very similar features, but differ in the one we want to test (the treatment). The new distributions will then be much less biased and we will be able to compare the effect of the treatment variable.\n\nConfounding variables\nMany factors can affect the analysis, among them:\n\nthe difficulty of each task, and\nthe strategy adopted to solve it.\n\nTo address the first, we naively quantify difficulty of a task as the minimum distance between the source and the target, as determined with the Floyed-Warshall algorithm, while the second is addressed through path length, e.g.Â the total number of links clicked by each player in the given game.\nControlling by such variables allows a first approximation to controlling these confounding effects.\n\n\nTreatment\nWe will test the effect of clicking mostly links in image boxes.\nThe question is then: given an equally difficult task assigned, and having a fixed number of possible choices to perform, are players more successful if they adopt clicking mainly links in images as a strategy?\n\n\nCode\n\nsuccessful_df = successful_df[(successful_df['shortest_path_length'].apply(lambda x: x == 3))]\nsuccessful_df = successful_df[(successful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\nsuccessful_df.head(3)\n\n\n\n\n\n\nCode\n\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['shortest_path_length'].apply(lambda x: x == 3))]\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\nunsuccessful_df.head(3)\n\n\nAt this point, we have filtered the successful and unsuccessful DataFrames by the following conditions:\nShortest path length = 3 --> we hypothesize that all paths have the same \"difficulty\" by enforcing this\nThe actual human path lengths are 3,4,5 --> the majority of human paths are within these lengths\nThe next step is to perform matching of datasets. To make this process possible, we first augment both successful and unsuccessful DataFrames with is_successful Boolean condition.\n\n\nCode\n\n\nsuccessful_df['is_successful'] = 1\nunsuccessful_df['is_successful'] = 0\n\n# merge the successful and unsuccessful DataFrames\nfinal_df = pd.concat([successful_df, unsuccessful_df])\nfinal_df.reset_index(drop=True, inplace=True)\n\nfinal_df.head(3)\n\n# save the DataFrame\nfinal_df.to_csv('final_df.csv')\n\n# next, we will calculate \"treatments\" involving semantic distance metrics for the DataFrame\n\n\n\n     \n\n\n2 . Document Similarity Similarity Between Wikipedia Articles: â€˜Bag-of-Wordsâ€™ Cosine Similarity Based on Word Frequencies\nWe are interested in measuring similarity between Wikipedia articles to investigate playersâ€™ strategies in Wikispeedia. The proposed similarity is Cosine Similarity measured on word frequencies given a pair of articles. Therefore, the assumption we make is that articles with similar occurrences of words are similar.\nIn order to mitigate similarity due to common English words such as â€œtheâ€, we apply a filter as follows:\nRemove all Stop words as defined in Scikit-learn\n\nhttps://scikit-learn.org/stable/modules/feature_extraction.html#nqy18\n\nRemove all white spaces and \\n characters\n\nApply a scaled version of word frequency count as implemented in TfidfTransformer in Scikit-learn:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\nThe cell blocks below show our data processing pipeline.\n\n\nCode\n\n\nimport os\n\nbase_path = '.data/plaintext_articles'\narticles = os.listdir(base_path)\n# sort the article names in alphabetical order. This is not strictly required\narticles = sorted(articles)\n\ndef remove_duplicates_and_fix_names(articles: list):\n    \"\"\"this function parses all the articles provided in the plaintext folder and stores the\n       names of all the articles after \"cleaning\" them (some articles containing accents have encoded names).\n       The raw text from the files are also stored in a list after removing white spaces and empty lines.\"\"\"\n    \n    def parse_text(article: str) -> str:\n        \"\"\"this helper function reads a raw text file and removes white spaces and empty lines.\"\"\"\n        with open(os.path.join(base_path, article)) as f:\n            # remove white spaces\n            raw_text = [line.rstrip() for line in f.readlines()]\n            # remove empty lines\n            raw_text = [line.strip() for line in raw_text if line != '']\n\n            return str(raw_text)\n    \n    # some article names have errors - fix these\n    article_names_cleaned, texts = [], []\n    \n    for article in articles:\n        # the \"%\" character indicates an encoded name\n        if \"%\" not in article:\n            # store the article name\n            article_names_cleaned.append(article)\n            # store the raw text from the article\n            texts.append(parse_text(article))\n            \n        else:\n            with open(os.path.join(base_path, article), 'r') as f:\n                # extract the correct name. It is always on the 3rd line of the article\n                correct_name = f.readlines()[2].replace('\\n', '')\n                # store the article name\n                article_names_cleaned.append(correct_name)\n                # store the raw text from the article\n                texts.append(parse_text(article))\n                    \n    \n    # remove \".txt\" from the article names\n    article_names_cleaned = [article.replace('.txt', '') for article in article_names_cleaned]\n    # remove list bracket from string casting of raw text\n    texts = [text.replace('[', '') for text in texts]\n    \n    return article_names_cleaned, texts\n\narticle_names_cleaned, texts = remove_duplicates_and_fix_names(articles)\n\n\n\n     \n\n\n\n\nCode\n\n# count word frequency using sklearn out-of-the-box functions\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport pandas as pd\nimport numpy as np\n\n# remove common stop words\ntfid_vectorizer = TfidfVectorizer(stop_words='english')\n# apply CountVectorizer and TfidfTransform sequentially\nmatrix = tfid_vectorizer.fit_transform(texts)\nmatrix = matrix.todense()\n\n# by default, \"TfidfVectorizer\" uses l2 norm and thus, to obtain the Cosine Similarity,\n# we simply perform a dot product of the matrix\ncosine_sim = pd.DataFrame(np.dot(np.array(matrix), np.array(matrix).T), columns=article_names_cleaned)\n# assign the index names also as the article names. This is used for easy querying pairs of articles\ncosine_sim.index = article_names_cleaned        \ncosine_sim\n\n\n\n\nCode\n\n\n# save the Cosine Similarity matrix so we do not have to re-compute it every time\nnp.save('cosine_similarity.npz', np.array(cosine_sim))\n\n\n\n     \n\n\n\n\nCode\n\nimport matplotlib.pyplot as plt\n\n# let's take a look at the Cosine Similarity distribution of the most similar article to a given article\nmost_similar = []\nfor idx in range(len(cosine_sim)):\n    # index [-2] because index [-1] is always = 1 since it is a self similarity\n    most_similar.append(sorted(cosine_sim.iloc[idx])[-2])\n    \n# plot the similarities\npd.Series(most_similar).plot(kind='hist', edgecolor='k')\nplt.title(\"Cosine Similarity Distribution of Most Similar Articles\")\nplt.xlabel(\"Cosine Similarity\"); plt.ylabel(\"Absolute Counts\")\n\n# there are no glaring red flags, e.g., most Cosine Similarities close to 0 or 1\n\n\n\n\nCode\n\n\n# show a few concrete examples and see if our metric makes empirical sense\ncosine_sim['Ukraine'].sort_values()\n\n# results look reasonable\n\n\n\n     \n\n\n\n\nCode\ncosine_sim['Zinc'].sort_values()\n# results also look reasonable\n\n\n\n\nCode\nimport seaborn as sns\n\n# \"Cygni\" are star systems/extrasolar planets and thus are expected to be similar\n# the time periods have some similarity\nsns.clustermap(cosine_sim.iloc[10:20, 10:20])\n\n\n\n\nCode\n\n\n# \"Cancri\" are exoplanets and thus are expected to be similar\n# we again see similarity between time periods\nsns.clustermap(cosine_sim.iloc[50:60, 50:60])\n\n\n\n     \n\n\n\n\nCode\n\n\n# after confirming that the Cosine Similarity method is informative, we next show the function\n# we will use to extract similarity of articles in a navigation path\n\n# first show a hard-coded navigation path for illustration\nnavigation_path = '14th_century;Time;Light;Rainbow'\n\n\n\n     \n\n\n\n\nCode\n\ndef get_path_cosine_similarity(path: str, matrix: pd.DataFrame, method: str='sequential') -> list:\n    \"\"\"this function takes a navigation path string and returns an array of the Cosine Similarity.\n       Two methods are supported: 'sequential' (default) which measures the sequential similarity and \n       'target' which measures the similarity between the current article at the target article.\n       \n       Input:\n       path: navigation path string\n       matrix: pre-computed Cosine Similarity matrix\n       method: measurement method (defaults to 'sequential')\n       \n       Return:\n       list: list of similarity measures\n    \"\"\"\n    \n    # there is 1 article that is duplicated in the plaintext folder. Problematically, the duplicated\n    # article has different names and navigation paths can point to these different names, yet the \n    # article is the same. This one exception is handled here. In the implementation, we do not have to \n    # worry about this particular article pointing back to itself as it is not possible\n    \n    # navigation paths are separated by \";\"\n    path = path.split(';')\n    \n    # if the path only contains the starting article, return None\n    if len(path) == 1:\n        return None\n    \n    path_sims = []\n    \n    def get_correct_name(query: str, base_path: str='.data/plaintext_articles') -> str:\n        \"\"\"this nested function is called in case a path contains error characters.\n           Returns the fixed path so it can be used to query the Cosine Similarity matrix.\"\"\"\n        if '_' in query and '%' not in query:\n            return query\n        elif query in ['Podcasting', 'Color', 'Fencing', 'Anemia', 'Quito']:\n            return query\n        else:\n            with open(os.path.join(base_path, f'{query}.txt'), 'r') as f:\n                out = f.readlines()[2].replace('\\n', '')\n                return out\n    \n    # compute the similarity between sequential nodes\n    # output list length is N-1\n    if method == 'sequential':\n        for idx in range(0, len(path)-1, 1):\n            try:\n                path_sims.append(matrix.loc[path[idx]].loc[path[idx+1]])\n            except Exception:\n                # either path or both paths could be wrong - just \"fix\" both\n                # treat the 1 exception due to duplicated file\n                if get_correct_name(path[idx]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = float(cosine_sim.loc['Polishâ€“Muscovite War (1605â€“1618)'].drop_duplicates()[get_correct_name(path[idx+1])])\n                    path_sims.append(sim)\n                    \n                elif get_correct_name(path[idx+1]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polishâ€“Muscovite War (1605â€“1618)']\n                    path_sims.append(sim)\n                    \n                else:\n                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[idx+1])])\n\n                    \n    # compute the similarity between the current node and the target node\n    # output list length is N\n    elif method == 'target':\n        for idx in range(0, len(path), 1):\n            try:\n                path_sims.append(matrix.loc[path[idx]].loc[path[-1]])\n            except Exception:\n                # either path or both paths could be wrong - just \"fix\" both\n                # treat the 1 exception due to duplicated file\n                if get_correct_name(path[idx]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = cosine_sim.loc['Polishâ€“Muscovite War (1605â€“1618)'].drop_duplicates()[get_correct_name(path[-1])]\n                    path_sims.append(sim)\n                    \n                elif get_correct_name(path[-1]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = float(cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polishâ€“Muscovite War (1605â€“1618)'])\n                    path_sims.append(sim)\n                    \n                else:\n                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[-1])])\n    \n    else:\n        raise ValueError('Unsupported similarity method: choose from \"sequential\" or \"target\".')\n        \n    return path_sims\n\n\n\n\nCode\nprint(navigation_path.split(';'))\n\n# show the \"sequential\" method\nsequential_sims = get_path_cosine_similarity(path=navigation_path,\n                                             matrix=cosine_sim,\n                                             method='sequential')\n# intepreted as:\n# sim(14th_century, Time) = 0.031\n# sim(Time, Light) = 0.147\n# sim(Light, Rainbow) = 0.158\n# the similarity list is always N-1 in length, N = # nodes\nsequential_sims\n\n\n\n\nCode\n\n\n# show the \"target\" method\ntarget_sims = get_path_cosine_similarity(path=navigation_path,\n                                         matrix=cosine_sim,\n                                         method='target')\n# intepreted as:\n# sim(14th_century, Rainbow) = 0.008\n# sim(Time, Rainbow) = 0.035\n# sim(Light, Rainbow) = 0.158\n# sim(Rainbow, Rainbow) = 1 (the path was successful so similarity is 1)\n# the similarity list is now N in length, N = # nodes\n\n# note: paths where the last index != 1 means the path was unsuccessful\ntarget_sims\n\n\n\n     \n\n\n\n\nCode\n# calculate the \"sequential\" and \"target\" cosine similarities and augment the DataFrame with these metrics\nfinal_df['path_seq_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim))\nfinal_df['path_target_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim, method='target'))\n\nfinal_df.head(3)\n\n\n\n\nCode\n\n# next, write a function that returns whether the Cosine similarities in a navigation path are increasing/decreasing\ndef sims_to_bool_path(path: str):\n    \"\"\"this function takes a navgiation path as input and returns the path with same dimensions\n       containing Booleans denoting whether the Cosine similarity is increasing\"\"\"\n    bool_path = []\n    for idx in range(len(path)-1):\n        if path[idx+1] > path[idx]:\n            bool_path.append(True)\n        else:\n            bool_path.append(False)\n    \n    return bool_path\n        \n\n\n\n\nCode\n\n\nfinal_df['path_seq_boolean'] = final_df['path_seq_cosine_sim'].apply(sims_to_bool_path)\nfinal_df['path_target_boolean'] = final_df['path_target_cosine_sim'].apply(sims_to_bool_path)\n\nfinal_df.head(3)\n\n\n\n     \n\n\n\n\nCode\n# this function filters paths that only contain strictly increasing Cosine similarities\ndef filter_strictly_increasing_sims(path_list: list):\n    return False if False in path_list else True\n\nfinal_df['seq_strictly_increasing'] = final_df['path_seq_boolean'].apply(filter_strictly_increasing_sims)\nfinal_df['target_strictly_increasing'] = final_df['path_target_boolean'].apply(filter_strictly_increasing_sims)\n\nfinal_df.head(3)\n\n\n\n\nCode\n\n\n# the matching algorithm requires some computation time\n# compute the matched DataFrames all at once and save them so they can be read into memory directly\n\n# there are 2 treatments\n#    1. strictly increasing *sequential* Cosine similarity\n#    2. strictly increasing *target* Cosine similarity\n\n# there are also 3 conditions we filter on\n#    1. human_path_length = 3\n#    2. human_path_length = 4\n#    3. human_path_length = 5\n\nimport networkx as nx\n\nfor treatment in ['seq_strictly_increasing', 'target_strictly_increasing']:\n    for length in [3,4,5]:\n    \n        treatment_df = final_df[final_df[treatment] == True]\n        control_df = final_df[final_df[treatment] == False]\n\n        G = nx.Graph()\n\n        for control_id, control_row in control_df.iterrows():\n            for treatment_id, treatment_row in treatment_df.iterrows():\n                # Adds an edge only for the same human path length\n                if (control_row['human_path_length'] == length and treatment_row['human_path_length'] == length):\n                    G.add_edge(control_id, treatment_id)\n\n        matching = nx.max_weight_matching(G)\n        matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n        balanced_df = final_df.iloc[matched]\n\n        # save balanced_df\n        balanced_df.to_csv(f'{treatment}_{length}.csv')\n\n\n\n     \n\n\n\n\nCode\n\n\n# next, perform logistic regression on each treatment with fixed human path length\n\n# read all the matched DataFrames into memory\ndf_seq_3 = pd.read_csv('seq_strictly_increasing_3.csv')\ndf_seq_4 = pd.read_csv('seq_strictly_increasing_4.csv')\ndf_seq_5 = pd.read_csv('seq_strictly_increasing_5.csv')\n\ndf_target_3 = pd.read_csv('target_strictly_increasing_3.csv')\ndf_target_4 = pd.read_csv('target_strictly_increasing_4.csv')\ndf_target_5 = pd.read_csv('target_strictly_increasing_5.csv')\n\n# store all results in a dictionary\nresults = {}\n\nimport statsmodels.formula.api as smf\n\nfor df, experiment_name in zip([df_seq_3, df_seq_4, df_seq_5, df_target_3, df_target_4, df_target_5],\n                               ['Sequential 3', 'Sequential 4', 'Sequential 5', 'Target 3', 'Target 4', 'Target 5']):\n    \n    if 'Sequential' in experiment_name:\n        mod = smf.logit(formula='is_successful ~  seq_strictly_increasing', data=df)\n    else:\n        mod = smf.logit(formula='is_successful ~  target_strictly_increasing', data=df)\n        \n    res = mod.fit()\n    \n    # dictionary to store the coefficients and p-values of the current experiment\n    curr_analysis = {}\n    # store the intercept\n    curr_analysis['Intercept'] = res.params[0]\n    # store the intercept p-value\n    curr_analysis['Intercept p-value'] = res.pvalues[0]\n    # store the coefficient for **strictly increasing Cosine similarity**\n    curr_analysis['Coefficient'] = res.params[1]\n    # store the coefficient p-value\n    curr_analysis['Coefficient p-value'] = res.pvalues[1]\n    \n    results[experiment_name] = curr_analysis\n    \nresults_df = pd.DataFrame(results)\n\n\n\n     \nresults_df\n\n\nThe syntax in the above table is interpreted as :\nSequential means the strictly increasing Cosine similarity was measured for sequential articles\nTarget means the strictly increasing Cosine similarity was measured based on sequential article compared to the target article\nThe number indicates the human path length fixed. Previously, we also fixed that the shortest path length is 3.\nOur research question is:\nWhen given X number of choices in the same difficulty game, how did successful and unsuccessful players differ in their clicking behaviour?\nBased on the above table, the p-values for both the incercept and coefficient for logistic regression are significant for all experiments except 1 (Sequential 5). The interpretation for all experiment findings are similar. Concretely, consider the experiment Sequential 3. The interpretation is:\n*Given a shortest path length of 3 (fixed difficulty) and given a human path length of 3 (when the human makes 3 choices on article clicks), clicking articles with increasing Cosine similarity between sequential articles (as a proxy to measure semantic meaning), leads to more successful outcomes.*\nFor Target experiments have a similar interpretation. Concretely, consider the experiment Target 5:\n*Given a shortest path length of 3 (fixed difficulty) and given a human path length of 5 (when the human makes 5 choices on article clicks), clicking articles with increasing Cosine similarity between the current article and the target article (as a proxy to measure semantic meaning), leads to more successful outcomes.*\nThese findings are interesting and suggest that (potentially contrarily to common sense) clicking articles based on semantic meaning can lead to more successful outcomes in the Wikispeedia game (at least the current version we are investigating).\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\nFigureÂ 1: ?(caption)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The ADAlyzers is a non-profit founded by 3 amazing students at EPFL."
  },
  {
    "objectID": "datastory.html#jeffs-plots-and-visualization",
    "href": "datastory.html#jeffs-plots-and-visualization",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "JEFFâ€™s PLOTS and VISUALIZATION",
    "text": "JEFFâ€™s PLOTS and VISUALIZATION\n\n\nCode\nfrom IPython.display import IFrame\nIFrame(\"images/path.pdf\", width=600, height=300)\n\n\n\n        \n        \n\n\n\n\nCode\n#from wand.image import Image as WImage\n#img = WImage(filename='images/path.pdf')"
  },
  {
    "objectID": "datastory.html#jeffs-plots",
    "href": "datastory.html#jeffs-plots",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "JEFFâ€™s PLOTS",
    "text": "JEFFâ€™s PLOTS\n\n\nCode\n\n\nfrom IPython.display import IFrame\nIFrame(\"images/path.png\", width=600, height=300)\n\n\n\n        \n        \n\n\n\n\nCode\n#from wand.image import Image as WImage\n#img = WImage(filename='images/path.pdf')"
  },
  {
    "objectID": "datastory.html#wikispeedia-what-does-x-have-to-do-with-y-you-better-find-out",
    "href": "datastory.html#wikispeedia-what-does-x-have-to-do-with-y-you-better-find-out",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Wikispeedia: What does X have to do with Y? You better find out!",
    "text": "Wikispeedia: What does X have to do with Y? You better find out!\nSince the earliest records, humans have wandered across endless lands and walked thorugh mysterious paths, in an attempt to uncover the unexplored and find better conditions for their tribes. The rise of agriculture and the industrial era has, however, changed things, turning us into bare sedentary animalsâ€¦ until now!\n\nWikispeedia is an easy and fun game: You are given two Wikipedia articles, and starting from the first article, your goal is to reach the second one exclusively by following links in the articles you encounter, effectively letting you explore your own paths across the vast wikipedia, for free! Your game data is collected by The EPFL Data Science Lab, which helps us and them better understand some aspects of human behaviour ðŸ˜‰.\nUsing this data, we ask ourselves, are there patterns in the way players navigate the network, that make them more successful? That is, are there any optimal strategy that doesnâ€™t require extensive knowledge of the network?\nPlayers are faced with multiple stimulae during the playing session, and playersâ€™ decisions as well as their posterior success may be influenced by them. For example, users might navigate the network through semantic similarity optimization, or they might just click the links found in images or tables, etc.\nDo these particular features have any effect on playersâ€™ success? Letâ€™s find out!"
  },
  {
    "objectID": "datastory.html#players-as-humans",
    "href": "datastory.html#players-as-humans",
    "title": "Marilyn Monroe and Relativity theory? Thereâ€™s always a connection! But howâ€¦?",
    "section": "Players as humans",
    "text": "Players as humans\n\nStrategies and attitudes\nBeing the Wikispeedia game and individual and timed game, players responses to the challenge it serves are miscellaneous. Individuals may consciously come up with a strategy to tackle the task they are assigned to, or unintentionally manifest a behavior that could be observed by scrupulous data analysts, like us. The Wikispeedia game itself was designed to extrapolate information allowing the computation of semantic distance between real-world concepts, to be leveraged in intelligent applications. Less ambitiously, weâ€™re interest in outlining players mutual attitudes and the likeliness these lead them to succeed in the game, potentially a causal relationship. Among the manifold performable strategies we hypothesised the following: - clicking on hyperlinks showing up at the top of the article page; - clicking on hyperlinks showing up in image captions; - clicking hyperlinks that are semantically related to the current article;\n- clicking hyperlinks that are semantically related to the target article.\nLook at the webpage below, isnâ€™t your gaze naturally directed towards those velvet red petals? This is what weâ€™re talking about.\n\n\n\nFigureÂ 1: An example article layout. Several elements pop to the eye like the images, do users click these more and, does this have an impact on their success?\n\n\nAs â€œthe Parthenonâ€™s facadeâ€ showing a marvellous â€œinterpretation of golden rectangles in its proportionsâ€ is judged to be aesthetically valuable by humans ability to discriminate at a sensory level, it is also true that visual content attracts human attention more than raw text.\nAdditionally, picture yourself in a competitive framework, with a stopwatch in your left hand and your right trembling fingers on a mouse, determined to break down the shortest ever recorded Wikispeedia game and be remembered for eternity. Wouldnâ€™t you click on top-positioned hyperlinks or would you parse the article in search for the best catch?\n\n\n\nFigureÂ 2: In a competitive framework, players have the additional stress of finishing the game fast; information in an easily diggestible format, like images, provide an easy and quick shortcut to progress.\n\n\n\nIndeed, this is the case, at least for the majority of players.\n\n\n\n\n\n\n\n(a) Players prefer clicking links in images.\n\n\n\n\n\n\n\n(b) Players prefer clicking links close to the top of the page.\n\n\n\n\nFigureÂ 3: What parts of the page are users most attracted to?\n\n\nThe plot FigureÂ 3 can be read as follows: the heights of the columns represent a measure of how trendy the target behavior is among players; the left-sided column, per behavior, is the distribution among unsuccessful players, while the right-sided is the one among successful players*.\nIt is evident that clicking hyperlinks showing up in image captions is a trendy behavior for Wikispeedia players, especially among the successful. The rate of successful players clicking on imagesâ€™ captions is roughly 4 times the unsuccessful players one.\nAgain, it is evident that clicking on hyperlinks showing up at the top of an article is largely trendier than clicking on hyperlinks elsewhere positioned in the article text (4 to 5 times more common). The gap between successful and unsuccesful players is not that large to show a pattern.\nHowever these are just distributions, i.e.Â a fair picture of reality, they show trends, no conclusions can be driven still.  *for a better understading of the graph (e.g.Â magnitudes of the column heights, units of measurements, etc.) see the notebook project_adalyzers.ipynb.\n\n\nEye-catcher hyperlinks: instinctual urge to be turned down or smart way to go?\nIn addition to this crispy question, we ask you, dear reader, to wait some time more for the statistical inference youâ€™re craving for, and dig a little deeper into human natural language structural traits.\nA little trickier to understand: semantic similarity. Semantic similarity is a metric that evaluates the distance between items on the basis of the likeness of their meaning or semantic content. Humans at play, challenged to reach a target article, whereof the only information theyâ€™re provided with is a word or phrase, display common sense knowledge. They would, in fact, look for semantic relatedness in clicking the hyperlinks. Thinking of the hypertextual Wikipedia graph alone, it can be viewed as a primitive semantic network: articles represent the concepts and the hyperlinks are the means of connection between them. Two articles can be more or less â€œsemantically relatedâ€ if, naivly, the two â€œoverlapâ€ well, with regard to the words present in them and the way they are ordered or used in context.\nAn illustration of the structure of the Wikispeedia hypertextual graph is shown in the image below (a part of it). The path between the source article â€œmathematicsâ€ to the target article â€œAlan Touringâ€ is pictured. The grey arrows connect nodes (i.e.Â hyperlinks) of a human path with length equal to 3, while the black arrows connect nodes of the shortest existing path between source and target articles, with length equal to 2.\n\n\n\nFigureÂ 4: There are many ways of solving a puzzle. The optimal (black) doesnâ€™t always make sense. A way for it to make sense is to click the article most similar to the current one (green path: sequential similarity increase), or click the article that is most similar to the target article (yellow path: target similarity increase). As shown by the intensity of the color, similarity increases in these paths as the player approaches the goal article.\n\n\n\n\nSemantic similarity: the paths that â€˜make senseâ€™.\nA reasonable strategy players may employ is to click on hyperlinks that are semantically closer and closer, along the human path, to the target article. The yellow and green arrows visualize these weighted conncetions, a.k.a. distances evaluated with the semantic similarity metric.\nSemantic similarity can be measured between neighbouring articles (yellow) or between the current article to the target article (green). The former is an unintuitive strategy as there is no guarantee that clicking articles with increasing semantic similarity to the previous article will help the player navigate to the target article. By constrast, the latter strategy follows common-sense, as navigating to articles that are increasingly similar to the target article should get you closer the target article.\nâ€œSemantic paths make sense, however, is this really the way the games are played? Is this actually effective w.r.t. success?â€\nLet us first validate whether measuring semantic similarity by comparing the frequency of word occurrences (bag-of-words cosine similarity) makes sense. Are all Wikispeedia articles just similar to each other (that would be problematic!)? Letâ€™s look at the distribution of cosine similarities between all articles compared to its most similar article.\n\n\n\nFigureÂ 5: The bag-of-words cosine similarities show a reasonable distribution. The proposed metric would be problematic if the distribution has heavy density near 0 and/or 1. The former would mean that all articles are semantically very different to all other articles. The latter would mean that all articles are semantically very similar to all other articles. Finally, there is also no heavy density at a particular value which would suggest the that navigating articles with a semantic strategy is meaningless as all articles have the same similarity.\n\n\nGreat, it looks like the measure of semantic similarity makes sense! Letâ€™s show a concrete example of the similarities!\n\n\n\nFigureÂ 6: The heatmap shows the cosine similarity between pairs of articles. Of course, on the diagonal, the similarties are all 1 as an articleâ€™s similarity to itself should be identical. The interesting values are around â€œCygniâ€ articles and time periods. â€œCygniâ€ are star systems/extrasolar planets so similarity in their corresponding Wikipedia articles should be expected. Similarly, time periods should also have some similarity, in agreement with the heatmap."
  },
  {
    "objectID": "datastory.html#graph-theoretical-features",
    "href": "datastory.html#graph-theoretical-features",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Graph-theoretical features",
    "text": "Graph-theoretical features\nTo describe a path, we can describe each link clicked (i.e.Â each piece of the path), in the context of the wikipedia network. What do we mean by this? Consider the articles â€œUnited States of Americaâ€ and â€œBernard Hinaultâ€. The first one is one of the countries with the most impact on the world, intuitively it has a lot of other articles pointing to it, and also links to a lot other articles.\nThe second isâ€¦ well, less known. Way less known. Our friend here is, although one of the greatest cyclist in history, part of a very narrow context, in the context of the wikipedia network. If we manage to reach â€œUnited States of Americaâ€, we have big chances of reaching any other article, as itâ€™s well connected, while if we ever reach â€œBernard Hinaultâ€, the paths we take may be longer.\nTo put this into numbers, we quantify the edge curvature of each edge (link) in the path. This quantifies the ammount of â€œflow potentialâ€ of a link. If a link has many ways in and many ways out, then its curvature is very negative, however if the ways in (or the ways out) are too few, we might have encountered a bottle neck, with a curvature closer to 0.\n  \\[\\begin{align*}\nF(_\\rightarrow e_\\rightarrow) &= 2 - in(i) - out(j)\n\\end{align*}\\]  \nUsing features based on this, we can now easily compare if two users took similar paths, regardless of which articles were given as a task; this allows a more general matching.\nIn order to remove the aforementioned confounders, the dataset is filtered and the matching is performed accordingly with the assumptions made. Players are matched on same shortest path distance of the assigned task, and maximum propensity score, with the treated subject being the user performing a specific strategy (e.g.Â clicking more frequently than the average on hyperlinks showing up in imagesâ€™ captions) and the controlled subject being the user NOT performing the strategy.\nThe analysis challenge can be reformulated as follows: â€œbeing assigned an equally difficult task and being equally free to perform a specific strategy, are players more successful if they adopt that specific strategy?â€"
  },
  {
    "objectID": "datastory.html#calculation-of-propensity-score",
    "href": "datastory.html#calculation-of-propensity-score",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Calculation of propensity score",
    "text": "Calculation of propensity score\nThe purpose motivating the matched analysis is to obtain a â€œbalancedâ€ testing dataset, with subjects actually comparable. In fact, we want to test potential correlation (causation ?) between specific strategies performed and playersâ€™ success. The players tested should then be at the same starting point when choosing what strategy to adopt, i.e.Â have same probability to get the treatment (a.k.a. propensity score).\nThe propensity scores are obtained by calculating the predicted outcomes of a logistic regression with the outcome being the probability of treatment and the predictors being features of the human paths. The treated subject is the user performing the specific strategy, the controlled subjct is the user NOT performing the specific startegy.\n\nLogistic regression\n\nFinally, letâ€™s test if thereâ€™s a programmatic way to win this game!!\nA logistic regression analysis is conducted with the outcome being playersâ€™ success in the game and the predictors being the strategies performed by the players.\n\n\nCode\ndef highlight_rows(row):\n    value = row.loc[\"p-value\"]\n    if value > 0.05:\n        color = '#FFB3BA' # Red\n    elif value < 0.05:\n        color = '#BAFFC9' # Green\n        \n    return ['background-color: {}'.format(color) for r in row]\n\nresults_df = (pd.read_csv(\"images/results_matching.csv\", index_col=0)\n              .loc[[\"Coefficient\", \"Coefficient p-value\"]]\n              .T)\n\nresults_df.columns = [\"Coefficient\", \"p-value\"]\n\n(results_df\n .sort_values(by=\"p-value\")\n .style\n .apply(highlight_rows, axis=1)\n)\n\n\n\n\n\n  \n    \n      Â \n      Coefficient\n      p-value\n    \n  \n  \n    \n      target_strictly_increasing\n      0.331702\n      0.000369\n    \n    \n      used_bottom\n      -0.480309\n      0.029248\n    \n    \n      seq_strictly_increasing\n      0.185502\n      0.335804\n    \n    \n      used_top\n      -0.187462\n      0.358632\n    \n    \n      used_center_bottom\n      -0.107463\n      0.604242\n    \n    \n      used_count_images\n      -0.016091\n      0.876319\n    \n  \n\n\n\nWe test 6 different possible strategies, and we find that two of them have some effect in the outcome, namely clicking links in the bottom of the page, and clicking to increase similarity to target (as p-values for both show thereâ€™s a significant difference between populations). They have, however, opposite effects: the former has a negative coefficient, while the latter has a positive one, implying that clicking links in the bottom of the page has a significantly negative effect on the outcome of the game. SO DONâ€™T DO THIS ðŸ˜­.\nAdditionally, non surprisingly images play no role in userâ€™s success.\nThe latter makes a lot of sense, though. This result also provides good evidence that thereâ€™s no shortcut to success in this game: If you wanna win in wikispeedia, you will (un)fortunately have to think hard!"
  },
  {
    "objectID": "datastory.html#a-closer-look-into-the-collected-data-hunt-for-covariates",
    "href": "datastory.html#a-closer-look-into-the-collected-data-hunt-for-covariates",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "A closer look into the collected data: hunt for covariates",
    "text": "A closer look into the collected data: hunt for covariates\n\nGraph-theoretical features\nTo characterize a path, we can describe each clicked link (i.e.Â each piece of the path, a.k.a. node of the graph), in the context of the Wikispeedia network. What do we mean by this? Consider the articles â€œUnited States of Americaâ€ and â€œBernard Hinaultâ€. The first one is one of most influential country on the world: intuitively it has a lot of other articles pointing to it, and also links to a lot other articles. The second isâ€¦ well, less known. Way less known. Our friend here is, although one of the greatest cyclist in history, part of a very narrow context, in the context of the Wikispeedia network.\nIf we manage to reach â€œUnited States of Americaâ€, we have big chances of reaching any other article, as itâ€™s well connected, while if we ever reach â€œBernard Hinaultâ€, the paths we take may be longer.\n\n\nCode\nImage(\"images/curv_example.png\", width=600, height=300)\n\n\n\n\n\nCurvature describes the context of the links that are clicked, within the network. If we click a link with a high curvature, weâ€™re in a very well connected part of the network. Matching by this quantity ensures that users are exploring similar regions of the wikipedia network, and thus allows us to make fair comparisons.\n\n\n\n\nTo put this into numbers, we quantify the curvature of each edge (clickable link in an article) in the path. This quantifies the ammount of â€œflow potentialâ€ of a node (article). If an article has many ways in and many ways out, then its curvature is very negative, however if the ways in (or the ways out) are too few, we might have encountered a bottle neck, with a curvature closer to 0.\nSo, if the input node is i, and the output node is j, the curvature is calculated as follows:\n  \\[\\begin{align*}\nF(_\\rightarrow e_\\rightarrow) &= 2 - in(i) - out(j)\n\\end{align*}\\]  \nAll the features listed make the Wikispeedia hypertextual graph clumpy and the addressed comparisons above unfair (we did it on purpose to show you that when your granny warned you not to judge people by their appearance, she was right!).\nIn statystical terms: these are observed covariates that we will leverage to remove the confounders that may affect the outcome of the logistic regression analysis weâ€™re aiming at conducting in order to find a potential correlation between the strategies/attitudes performed by players and their success.\nTo state it more clearly, the shortest path distance between the source and target article in the Wikispeedia hyperlinks graph can be viewed as a measure of the â€œdifficultyâ€ of the randomly assigned task (source and target articles), affecting both the playersâ€™ successfulness and the strategy adopted to address it (a.k.a. confounder); this is a static feature. Furthermore, depending on the usersâ€™ choices, the task can become harder or easier as the game develops (dynamic feature): this trait can be captured by the, path dependent, features hyperlinksâ€™ in- and out- degrees.\nAs we wish to compare only similar paths, these features will be exploited to restrict the inquiry on paths with a certain degree of â€œsimilarityâ€ between themselves."
  },
  {
    "objectID": "datastory_jeff.html#players-as-humans",
    "href": "datastory_jeff.html#players-as-humans",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Players as humans",
    "text": "Players as humans\n\nStrategies and attitudes\nBeing the Wikispeedia game and individual and timed game, players responses to the challenge it serves are miscellaneous. Individuals may consciously come up with a strategy to tackle the task they are assigned to, or unintentionally manifest a behavior that could be observed by scrupulous data analysts, like us. The Wikispeedia game itself was designed to extrapolate information allowing the computation of semantic distance between real-world concepts, to be leveraged in intelligent applications. Less ambitiously, weâ€™re interest in outlining players mutual attitudes and the likeliness these lead them to succeed in the game, potentially a causal relationship. Among the manifold performable strategies we hypothesised the following: - clicking on hyperlinks showing up at the top of the article page; - clicking on hyperlinks showing up in image captions; - clicking hyperlinks that are semantically related to the current article;\n- clicking hyperlinks that are semantically related to the target article.\nLook at the webpage below, isnâ€™t your gaze naturally directed towards those velvet red petals? This is what weâ€™re talking about.\n\n\n\nFigureÂ 1: An example article layout. Several elements pop to the eye like the images, do users click these more and, does this have an impact on their success?\n\n\nAs â€œthe Parthenonâ€™s facadeâ€ showing a marvellous â€œinterpretation of golden rectangles in its proportionsâ€ is judged to be aesthetically valuable by humans ability to discriminate at a sensory level, it is also true that visual content attracts human attention more than raw text.\nAdditionally, picture yourself in a competitive framework, with a stopwatch in your left hand and your right trembling fingers on a mouse, determined to break down the shortest ever recorded Wikispeedia game and be remembered for eternity. Wouldnâ€™t you click on top-positioned hyperlinks or would you parse the article in search for the best catch?\n\n\n\nFigureÂ 2: In a competitive framework, players have the additional stress of finishing the game fast; information in an easily diggestible format, like images, provide an easy and quick shortcut to progress.\n\n\n\nIndeed, this is the case, at least for the majority of players.\n\n\n\n\n\n\n\n(a) Players prefer clicking links in images.\n\n\n\n\n\n\n\n(b) Players prefer clicking links close to the top of the page.\n\n\n\n\nFigureÂ 3: What parts of the page are users most attracted to?\n\n\nThe plot FigureÂ 3 can be read as follows: the heights of the columns represent a measure of how trendy the target behavior is among players; the left-sided column, per behavior, is the distribution among unsuccessful players, while the right-sided is the one among successful players*.\nIt is evident that clicking hyperlinks showing up in image captions is a trendy behavior for Wikispeedia players, especially among the successful. The rate of successful players clicking on imagesâ€™ captions is roughly 4 times the unsuccessful players one.\nAgain, it is evident that clicking on hyperlinks showing up at the top of an article is largely trendier than clicking on hyperlinks elsewhere positioned in the article text (4 to 5 times more common). The gap between successful and unsuccesful players is not that large to show a pattern.\nHowever these are just distributions, i.e.Â a fair picture of reality, they show trends, no conclusions can be driven still.  *for a better understading of the graph (e.g.Â magnitudes of the column heights, units of measurements, etc.) see the notebook project_adalyzers.ipynb.\n\n\nEye-catcher hyperlinks: instinctual urge to be turned down or smart way to go?\nIn addition to this crispy question, we ask you, dear reader, to wait some time more for the statistical inference youâ€™re craving for, and dig a little deeper into human natural language structural traits.\nA little trickier to understand: semantic similarity. Semantic similarity is a metric that evaluates the distance between items on the basis of the likeness of their meaning or semantic content. Humans at play, challenged to reach a target article, whereof the only information theyâ€™re provided with is a word or phrase, display common sense knowledge. They would, in fact, look for semantic relatedness in clicking the hyperlinks. Thinking of the hypertextual Wikipedia graph alone, it can be viewed as a primitive semantic network: articles represent the concepts and the hyperlinks are the means of connection between them. Two articles can be more or less â€œsemantically relatedâ€ if, naivly, the two â€œoverlapâ€ well, with regard to the words present in them and the way they are ordered or used in context.\nAn illustration of the structure of the Wikispeedia hypertextual graph is shown in the image below (a part of it). The path between the source article â€œmathematicsâ€ to the target article â€œAlan Touringâ€ is pictured. The grey arrows connect nodes (i.e.Â hyperlinks) of a human path with length equal to 3, while the black arrows connect nodes of the shortest existing path between source and target articles, with length equal to 2.\n\n\n\nFigureÂ 4: There are many ways of solving a puzzle. The optimal (black) doesnâ€™t always make sense. A way for it to make sense is to click the article most similar to the current one (green path: sequential similarity increase), or click the article that is most similar to the target article (yellow path: target similarity increase). As shown by the intensity of the color, similarity increases in these paths as the player approaches the goal article.\n\n\n\n\nSemantic similarity: the paths that â€˜make senseâ€™.\nA reasonable strategy players may employ is to click on hyperlinks that are semantically closer and closer, along the human path, to the target article. The yellow and green arrows visualize these weighted conncetions, a.k.a. distances evaluated with the semantic similarity metric.\nSemantic similarity can be measured between neighbouring articles (yellow) or between the current article to the target article (green). The former is an unintuitive strategy as there is no guarantee that clicking articles with increasing semantic similarity to the previous article will help the player navigate to the target article. By constrast, the latter strategy follows common-sense, as navigating to articles that are increasingly similar to the target article should get you closer the target article.\nâ€œSemantic paths make sense, however, is this really the way the games are played? Is this actually effective w.r.t. success?â€\nLet us first validate whether measuring semantic similarity by comparing the frequency of word occurrences (bag-of-words cosine similarity) makes sense. Are all Wikispeedia articles just similar to each other (that would be problematic!)? Letâ€™s look at the distribution of cosine similarities between all articles compared to its most similar article.\n\n\n\nFigureÂ 5: The bag-of-words cosine similarities show a reasonable distribution. The proposed metric would be problematic if the distribution has heavy density near 0 and/or 1. The former would mean that all articles are semantically very different to all other articles. The latter would mean that all articles are semantically very similar to all other articles. Finally, there is also no heavy density at a particular value which would suggest the that navigating articles with a semantic strategy is meaningless as all articles have the same similarity.\n\n\nGreat, it looks like the measure of semantic similarity makes sense! Letâ€™s show a concrete example of the similarities!\n\n\n\nFigureÂ 6: The heatmap shows the cosine similarity between pairs of articles. Of course, on the diagonal, the similarties are all 1 as an articleâ€™s similarity to itself should be identical. The interesting values are around â€œCygniâ€ articles and time periods. â€œCygniâ€ are star systems/extrasolar planets so similarity in their corresponding Wikipedia articles should be expected. Similarly, time periods should also have some similarity, in agreement with the heatmap."
  },
  {
    "objectID": "datastory_jeff.html#a-look-into-the-collected-data",
    "href": "datastory_jeff.html#a-look-into-the-collected-data",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "A look into the collected dataâ€¦",
    "text": "A look into the collected dataâ€¦\nWhat data do we have though? How hard are the tasks our brave users had to overcome? Letâ€™s quickly explore this.\n\n\nCode\n# take a look at the shortest_path distributions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nfinal_df = pd.read_csv('processed/final_df_curv_feats.csv')\n\n\nfig, ax = plt.subplots(2,1, \n                       figsize=(10,8), \n                       gridspec_kw={\"hspace\":0.3})\nplt.rcParams[\"font.size\"] = 12\n\n\nsns.barplot(data=(final_df\n                  .groupby([\"shortest_path_length\", \n                            \"is_successful\"])['path']\n                  .count()\n                  .reset_index()\n                 ),\n            y=\"path\", \n            x=\"shortest_path_length\", \n            hue=\"is_successful\",\n            ax=ax[0], \n            palette=[\"#b2df8a\", \"#1f78b4\"])\n\n\nax[0].set_ylabel('Absolute Counts'); \nax[0].set_xlabel('Shortest Path');\nax[0].set_title(\"1. Optimal lengths of the proposed games.\")\n#Count of human paths, stratified by shortest possible path\")\n\nax[0].legend(loc=1)\nax[0].set_ylabel('Absolute Counts') \nax[0].set_xlabel('Shortest Path')\n\n\nsns.barplot(data=(final_df.query(\"shortest_path_length==4\")\n                  .groupby([\"human_path_length\", \n                            \"is_successful\"])['path']\n                  .count()\n                  .reset_index()\n                 ),\n            y=\"path\", \n            x=\"human_path_length\", \n            hue=\"is_successful\",\n            ax=ax[1], \n            palette=[\"#b2df8a\", \"#1f78b4\"])\n\nax[1].set_xlim(-0.5,10.5)\n\nax[1].set_ylabel('Absolute Counts')\nax[1].set_xlabel('Human path length')\nax[1].set_title(\"2. Path lengths obtained by human players, when optimal length is 4.\")\n# Count of human path lengths, with shortest possible path = 3\");\n\nax[1].legend(loc=1);\n\n\n\n\n\nHow are the data distributed among successful and unsuccessful games? We show: (top) The distributions of optimal lengths of the proposed games, and (bottom) The distributions of path lengths obtained by humans.\n\n\n\n\nThe graphs reveal some interesting features of the Wikispeedia hypertextual graph and the humans navigability network. The first plot shows the distribution of the shortest path distance of the tasks assigned to players.\nThe second plot shows the length of the actual paths taken by humans, on the subset with shortest path distance equal to 4. Given that the shortest path distance is taken as a proxy to game difficulty (see Section A matching study): - the distribution of the difficulty of games peaks at length equal to 3 and ranges from 2 to 5; - there exist hyperlinks which are not connected to any other; - as difficulty increases, the class unbalance decreases; - the distribution of successful human paths (with fixed shortest path distance == 4) spreads to a wide range of values, non monotously. - the distribution of unsuccessful paths (with fixed shortest path distance == 4) is severely skewed to the left and peaks at 3, meaning that in such paths, players give up before even having a chance to win.\nMore down-to-Earth: the distribution of optimal path lengths and the distribution of human path lengths are uneven and significantly vary for the populations of successful vs unsuccessful players.  These conclusions bring us to look for features of the games possibly affecting players performances (a.k.a. covariates), leading them to behave so inhomogeneously."
  },
  {
    "objectID": "datastory.html",
    "href": "datastory.html",
    "title": "Marilyn Monroe and Relativity theory? Thereâ€™s always a connection! But howâ€¦?",
    "section": "",
    "text": "Since the earliest records, humans have wandered across endless lands and walked thorugh mysterious paths, in an attempt to uncover the unexplored and find better conditions for their tribes. The rise of agriculture and the industrial era has, however, changed things, turning us into bare sedentary animalsâ€¦ until now!\nWikispeedia is an easy and fun game: you are given two Wikipedia articles, and starting from the first article, your goal is to reach the second one exclusively by clicking links in the articles you encounter, effectively letting you explore your own paths across the vast Wikipedia network, for free! Your game data is collected by The EPFL Data Science Lab, which helps us and them better understand some aspects of human behaviour ðŸ˜‰.\nUsing this data, we ask ourselves, are there patterns in the way players navigate the network, that make them more successful? That is, is there any optimal strategy that doesnâ€™t require extensive knowledge of the network?\nPlayers are faced with multiple stimuli during the playing session, and playersâ€™ decisions as well as their subsequent success may be influenced by them. For example, users might navigate the network through semantic similarity optimization, or they might just click the links found in images or tables, etc.\nDo these particular features have any effect on playersâ€™ success? Letâ€™s find out!"
  }
]