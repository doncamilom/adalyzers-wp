[
  {
    "objectID": "datastory.html#wikispeedia-what-the-hell-is-this",
    "href": "datastory.html#wikispeedia-what-the-hell-is-this",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Wikispeedia: What the hell is this?",
    "text": "Wikispeedia: What the hell is this?\nSince the earliest records, humans have wandered across endless lands and walked thorugh mysterious paths, in an attempt to uncover the unexplored and find better conditions for their tribes. The rise of agriculture and the industrial era has, however, changed things, turning us into bare sedentary animals… until now!\n\nWikispeedia is an easy and fun game: You are given two Wikipedia articles, and starting from the first article, your goal is to reach the second one exclusively by following links in the articles you encounter, effectively letting you explore your own paths across the vast wikipedia, for free! Your game data is collected by The EPFL Data Science Lab, which helps us and them better understand some aspects of human behaviour 😉.\nUsing this data, we ask ourselves, are there patterns in the way players navigate the network, that make them more successful? That is, are there any optimal strategy that doesn’t require extensive knowledge of the network?\nPlayers are faced with multiple stimulae during the playing session, and players’ decisions as well as their posterior success may be influenced by them. For example, users might navigate the network through semantic similarity optimization, or they might just click the links found in images or tables, etc.\nDo these particular features have any effect on players’ success? Let’s see!"
  },
  {
    "objectID": "datastory.html#a-look-into-the-collected-data",
    "href": "datastory.html#a-look-into-the-collected-data",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "A look into the collected data…",
    "text": "A look into the collected data…\n\n\nCode\nimport pandas as pd\nimport os\n\nparent_folder_path = '.data/wikispeedia_paths-and-graph/'\n\npaths_finished_df = (pd.read_csv(os.path.join(parent_folder_path, 'paths_finished.tsv'), \n                                 sep='\\t', skiprows=15, header=None)\n                     .rename(columns={0:\"ip\",\n                                      1:\"timestamp\",\n                                      2:\"duration\",\n                                      3:\"path\",\n                                      4:\"rating\"}))\n\npaths_unfinished_df=(pd.read_csv(os.path.join(parent_folder_path, 'paths_unfinished.tsv'), \n                               sep='\\t', skiprows=16, header=None)\n                     .rename(columns={0:\"ip\",\n                                      1:\"timestamp\",\n                                      2:\"duration\",\n                                      3:\"path\",\n                                      4:\"target\",\n                                      5:\"type\"}))\n\n# Read the shortest path matrix\nshortest_path_matrix = []\nwith open('.data/wikispeedia_paths-and-graph/shortest-path-distance-matrix.txt', 'r') as f:\n    # the first 17 lines (indexed from 0) is the file description \n    for line in f.readlines()[17:]:\n        shortest_path_matrix.append(line)\n\n        \n# next, we need a list of all the article names. The order of the articles \n# is the same as the shortest_path_matrix as per the file descriptions\narticles = pd.read_csv('.data/wikispeedia_paths-and-graph/articles.tsv', \n                       sep='\\t', \n                       skiprows=11, \n                       header=None)[0]\narticle_names_cleaned = []\n\n# NOTE TO GINEVRA:\n# I need to change the function below to use urlparse\n\n# some articles have encoded names. Decode these\nfor article in articles:\n    # the \"%\" character indicates an encoded name\n    if \"%\" not in article:\n        # store the article name\n        article_names_cleaned.append(article)\n    else:\n        with open(os.path.join('.data/plaintext_articles', f'{article}.txt'), 'r') as f:\n            # extract the correct name. It is always on the 3rd line of the article\n            correct_name = f.readlines()[2].replace('\\n', '')\n            # store the article name\n            article_names_cleaned.append(correct_name)\n\narticle_names_cleaned[:3]\n\n\n# for each human path, perform the following steps:\n#      1. extract the source and target article\n#      2. find the *index* in the article names list that corresponds to the source and target article\n#      3. the corresponding *index* row in the shortest path matrix corresponds to the source article. \n#         from this list of numbers, use the target article *index* to find the *shortest path length*\n\ndef augment_with_shortest_path(df: pd.DataFrame, successful: bool) -> pd.DataFrame:\n    \"\"\"\n    this function takes a Series and returns a DataFrame with the following columns:\n       1. path\n       2. source article\n       3. target article\n       4. shortest path length\n       \n    input:\n       df: the Pandas DataFrame containing all the human navigation paths\n       successful: a boolean indicating whether the paths were successful or not\n    \"\"\"\n    # remove all paths with back-tracks\n    df = df[~df['path'].str.contains('<')]\n    \n    def clean_query_article_name(name: str) -> str:\n        \"\"\"this helper function takes an encoded article name and returns the cleaned name\"\"\"\n        if \"%\" in name:\n            with open(os.path.join('.data/plaintext_articles', f'{name}.txt'), 'r') as f:\n                # extract the correct name. It is always on the 3rd line of the article\n                return f.readlines()[2].replace('\\n', '')    \n        else:\n            return name\n        \n    paths, human_path_lengths, source_articles, target_articles, shortest_paths = [], [], [], [], []\n    \n    # all information required for successful paths is in the path itself\n    if successful:\n        for human_path in df['path']:\n            paths.append(human_path)\n\n            split_path = human_path.split(';')\n            # subtract 1 because we do not count the source article\n            human_path_lengths.append(len(split_path)-1)\n\n            source = clean_query_article_name(split_path[0])\n            target = clean_query_article_name(split_path[-1])\n            source_articles.append(source)\n            target_articles.append(target)\n    \n    # unsuccessful paths require extraction of the target article from a separate column\n    else:\n        for human_path, target in zip(df['path'], df['target']):\n            paths.append(human_path)\n            \n            split_path = human_path.split(';')\n            # subtract 1 because we do not count the source article\n            human_path_lengths.append(len(split_path)-1)\n\n            source = clean_query_article_name(split_path[0])\n            source_articles.append(source)\n            target_articles.append(target)\n        \n    for source, target in zip(source_articles, target_articles):\n        source_index = article_names_cleaned.index(source)\n        # there are target articles that were not provided in the plain text files\n        try:\n            target_index = article_names_cleaned.index(target)\n        except Exception:\n            shortest_paths.append(\"N/A\")\n            continue\n            \n        # query the shortest path matrix to get the correct vector (corresponding to the source article)\n        shortest_path_vector = shortest_path_matrix[source_index]\n        # now find the target article indexed integer in the vector\n        shortest = shortest_path_vector[target_index]\n        # it's not always possible to get to the target article. Impossible navigation is denoted by \"_\"\n        if shortest == \"_\":\n            shortest_paths.append(\"Impossible\")\n        else:\n            shortest_paths.append(int(shortest))\n        \n        \n    # create the augmented DataFrame\n    out = pd.DataFrame({\n                      'path': paths,\n                      'source_article': source_articles,\n                      'target_article': target_articles,\n                      'human_path_length': human_path_lengths,\n                      'shortest_path_length': shortest_paths\n                      })\n    \n    return out\n\nsuccessful_df = augment_with_shortest_path(df=paths_finished_df, successful=True)\n# to investigate human behaviour, we remove all \"Impossible paths\" and also shortest_path_length = 0\nsuccessful_df = successful_df[(successful_df['shortest_path_length'].apply(lambda x: x != 'Impossible' and x != 0))]\n# next, we will keep only shortest_path_lengths >= 3\nsuccessful_df = successful_df[(successful_df['shortest_path_length'].apply(lambda x: x >= 3))]\n\n\n\nunsuccessful_df = augment_with_shortest_path(df=paths_unfinished_df, successful=False)\n\n# some target articles for unsuccessful paths were not provided in the plain_text folder, denoted in\n# the DataFrame as \"N/A\". Remove these\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['shortest_path_length'].apply(lambda x: x != 'N/A' and x != 'Impossible'))]\n# some unsuccesful paths only contain 1 article because the user didn't click anything\n# these are less meaningful paths to analyze as we are interested in users who tried but failed\n# we will keep paths where the human clicked at least 3 articles\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['shortest_path_length'].apply(lambda x: x >= 3))]\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['human_path_length'].apply(lambda x: x >= 3))]\n\n\nHow are the data distributed among successful and unsuccessful games? The following plots show\n\nThe distributions of optimal lengths of the proposed games.\nThe distributions of path lengths obtained by humans.\n\n\n\nCode\n# take a look at the shortest_path distributions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2,1, \n                       figsize=(10,8), \n                       gridspec_kw={\"hspace\":0.4})\nplt.rcParams[\"font.size\"] = 12\n\n\nsuccessful_counts = (successful_df['shortest_path_length']\n                     .value_counts()\n                     .reset_index())\nsuccessful_counts[\"success\"] = \"successful\"\n\nunsuccessful_counts = (unsuccessful_df['shortest_path_length']\n                       .value_counts()\n                       .reset_index())\nunsuccessful_counts[\"success\"] = \"unsuccessful\"\n\nall_counts = pd.concat([successful_counts, \n                        unsuccessful_counts], \n                       axis=0)\n\nsns.barplot(data=all_counts, \n            x=\"index\", \n            y=\"shortest_path_length\", \n            hue=\"success\",\n            ax=ax[0], \n            palette=[\"#b2df8a\", \"#1f78b4\"])\n\nax[0].set_ylabel('Absolute Counts'); \nax[0].set_xlabel('Shortest Path');\nax[0].set_title(\"1. Optimal lengths of the proposed games.\")\n#Count of human paths, stratified by shortest possible path\")\n\nax[0].legend(loc=1)\nax[0].set_ylabel('Absolute Counts') \nax[0].set_xlabel('Shortest Path')\n\n\n\n# Next plot: frequency by human length\nsuccessful_human_path_lengths_3 = (successful_df\n                                   [successful_df['shortest_path_length'] == 3]\n                                   ['human_path_length']\n                                   .value_counts()\n                                   .reset_index())\n\nsuccessful_human_path_lengths_3[\"success\"] = \"successful\"\n\nunsuccessful_human_path_lengths_3 = (unsuccessful_df\n                                     [unsuccessful_df['shortest_path_length'] == 3]\n                                     ['human_path_length']\n                                     .value_counts()\n                                     .reset_index())\n\nunsuccessful_human_path_lengths_3[\"success\"] = \"unsuccessful\"\n\nall_path_l3 = pd.concat([successful_human_path_lengths_3,\n                         unsuccessful_human_path_lengths_3])\n\nsns.barplot(data=all_path_l3, \n            x=\"index\", \n            y=\"human_path_length\", \n            hue=\"success\", \n            ax=ax[1],\n            palette=[\"#b2df8a\", \"#1f78b4\"])\n\nax[1].set_xlim(-0.5,10.5)\n\nax[1].set_ylabel('Absolute Counts')\nax[1].set_xlabel('Human path length')\nax[1].set_title(\"2. Path lengths obtained by human players, when optimal length is 3.\")\n# Count of human path lengths, with shortest possible path = 3\");\n\nax[1].legend(loc=1);\n\n\n\n\n\nGiven a shortest path length of 3, the majority of human path lengths are 3, 4, 5 for both successful and unsuccessful paths. Let’s filter the corresponding DataFrames to only keep these path lengths. The matched analysis will be done with these lengths."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The ADAlyzers is a non-profit founded by 3 amazing students at EPFL."
  },
  {
    "objectID": "datastory.html#and-now-the-real-deal",
    "href": "datastory.html#and-now-the-real-deal",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "And now the real deal",
    "text": "And now the real deal\nWe wanna test wheather different strategies lead to success in the Wikispeedia game. With this goal in mind, paths can be characterized through local features of the clicked links and articles. The features we extract are:\n\nRelative position of each hyperlink within the current article.\nIs the hyperlink next to an image?\nIs the hyperlink inside a table?\nSimilarity between current and target article.\nHyperlink curvature along the path.\n\nOnce each path has been featurized we perform a matching analysis that lets us reduce the bias in the distributions of successful and unsuccessful populations. For this, we find pairs of subjects (paths) in the dataset, that have very similar features, but differ in the one we want to test (the treatment). The new distributions will then be much less biased and we will be able to compare the effect of the treatment variable.\n\nConfounding variables\nMany factors can affect the analysis, among them:\n\nthe difficulty of each task, and\nthe strategy adopted to solve it.\n\nTo address the first, we naively quantify difficulty of a task as the minimum distance between the source and the target, as determined with the Floyed-Warshall algorithm, while the second is addressed through path length, e.g. the total number of links clicked by each player in the given game.\nControlling by such variables allows a first approximation to controlling these confounding effects.\n\n\nTreatment\nWe will test the effect of clicking mostly links in image boxes.\nThe question is then: given an equally difficult task assigned, and having a fixed number of possible choices to perform, are players more successful if they adopt clicking mainly links in images as a strategy?\n\n\nCode\n\nsuccessful_df = successful_df[(successful_df['shortest_path_length'].apply(lambda x: x == 3))]\nsuccessful_df = successful_df[(successful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\nsuccessful_df.head(3)\n\n\n\n\n\n\n\n\n  \n    \n      \n      path\n      source_article\n      target_article\n      human_path_length\n      shortest_path_length\n    \n  \n  \n    \n      1\n      14th_century;Europe;Africa;Atlantic_slave_trad...\n      14th_century\n      African_slave_trade\n      4\n      3\n    \n    \n      5\n      14th_century;Europe;North_America;United_State...\n      14th_century\n      John_F._Kennedy\n      5\n      3\n    \n    \n      6\n      14th_century;China;Gunpowder;Fire\n      14th_century\n      Fire\n      3\n      3\n    \n  \n\n\n\n\n\n\nCode\n\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['shortest_path_length'].apply(lambda x: x == 3))]\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\nunsuccessful_df.head(3)\n\n\n\n\n\n\n  \n    \n      \n      path\n      source_article\n      target_article\n      human_path_length\n      shortest_path_length\n    \n  \n  \n    \n      5\n      Agriculture;History_of_the_world;China;Yangtze...\n      Agriculture\n      Grand_Canal_of_China\n      3\n      3\n    \n    \n      13\n      Ape;Asia;Computer;Automobile\n      Ape\n      Tin\n      3\n      3\n    \n    \n      14\n      Symmetry;Science;Age_of_Enlightenment;Dark_Age...\n      Symmetry\n      Scottish_Episcopal_Church\n      5\n      3\n    \n  \n\n\n\n\nAt this point, we have filtered the successful and unsuccessful DataFrames by the following conditions:\nShortest path length = 3 --> we hypothesize that all paths have the same \"difficulty\" by enforcing this\nThe actual human path lengths are 3,4,5 --> the majority of human paths are within these lengths\nThe next step is to perform matching of datasets. To make this process possible, we first augment both successful and unsuccessful DataFrames with is_successful Boolean condition.\n\n\nCode\n\n\nsuccessful_df['is_successful'] = 1\nunsuccessful_df['is_successful'] = 0\n\n# merge the successful and unsuccessful DataFrames\nfinal_df = pd.concat([successful_df, unsuccessful_df])\nfinal_df.reset_index(drop=True, inplace=True)\n\nfinal_df.head(3)\n\n# save the DataFrame\nfinal_df.to_csv('final_df.csv')\n\n# next, we will calculate \"treatments\" involving semantic distance metrics for the DataFrame\n\n\n\n     \n\n\n2 . Document Similarity Similarity Between Wikipedia Articles: ‘Bag-of-Words’ Cosine Similarity Based on Word Frequencies\nWe are interested in measuring similarity between Wikipedia articles to investigate players’ strategies in Wikispeedia. The proposed similarity is Cosine Similarity measured on word frequencies given a pair of articles. Therefore, the assumption we make is that articles with similar occurrences of words are similar.\nIn order to mitigate similarity due to common English words such as “the”, we apply a filter as follows:\nRemove all Stop words as defined in Scikit-learn\n\nhttps://scikit-learn.org/stable/modules/feature_extraction.html#nqy18\n\nRemove all white spaces and \\n characters\n\nApply a scaled version of word frequency count as implemented in TfidfTransformer in Scikit-learn:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\nThe cell blocks below show our data processing pipeline.\n\n\nCode\n\n\nimport os\n\nbase_path = '.data/plaintext_articles'\narticles = os.listdir(base_path)\n# sort the article names in alphabetical order. This is not strictly required\narticles = sorted(articles)\n\ndef remove_duplicates_and_fix_names(articles: list):\n    \"\"\"this function parses all the articles provided in the plaintext folder and stores the\n       names of all the articles after \"cleaning\" them (some articles containing accents have encoded names).\n       The raw text from the files are also stored in a list after removing white spaces and empty lines.\"\"\"\n    \n    def parse_text(article: str) -> str:\n        \"\"\"this helper function reads a raw text file and removes white spaces and empty lines.\"\"\"\n        with open(os.path.join(base_path, article)) as f:\n            # remove white spaces\n            raw_text = [line.rstrip() for line in f.readlines()]\n            # remove empty lines\n            raw_text = [line.strip() for line in raw_text if line != '']\n\n            return str(raw_text)\n    \n    # some article names have errors - fix these\n    article_names_cleaned, texts = [], []\n    \n    for article in articles:\n        # the \"%\" character indicates an encoded name\n        if \"%\" not in article:\n            # store the article name\n            article_names_cleaned.append(article)\n            # store the raw text from the article\n            texts.append(parse_text(article))\n            \n        else:\n            with open(os.path.join(base_path, article), 'r') as f:\n                # extract the correct name. It is always on the 3rd line of the article\n                correct_name = f.readlines()[2].replace('\\n', '')\n                # store the article name\n                article_names_cleaned.append(correct_name)\n                # store the raw text from the article\n                texts.append(parse_text(article))\n                    \n    \n    # remove \".txt\" from the article names\n    article_names_cleaned = [article.replace('.txt', '') for article in article_names_cleaned]\n    # remove list bracket from string casting of raw text\n    texts = [text.replace('[', '') for text in texts]\n    \n    return article_names_cleaned, texts\n\narticle_names_cleaned, texts = remove_duplicates_and_fix_names(articles)\n\n\n\n     \n\n\n\n\nCode\n\n# count word frequency using sklearn out-of-the-box functions\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport pandas as pd\nimport numpy as np\n\n# remove common stop words\ntfid_vectorizer = TfidfVectorizer(stop_words='english')\n# apply CountVectorizer and TfidfTransform sequentially\nmatrix = tfid_vectorizer.fit_transform(texts)\nmatrix = matrix.todense()\n\n# by default, \"TfidfVectorizer\" uses l2 norm and thus, to obtain the Cosine Similarity,\n# we simply perform a dot product of the matrix\ncosine_sim = pd.DataFrame(np.dot(np.array(matrix), np.array(matrix).T), columns=article_names_cleaned)\n# assign the index names also as the article names. This is used for easy querying pairs of articles\ncosine_sim.index = article_names_cleaned        \ncosine_sim\n\n\n\n\n\n\n  \n    \n      \n      Áedán mac Gabráin\n      Åland\n      Édouard Manet\n      Éire\n      Óengus I of the Picts\n      €2 commemorative coins\n      10th_century\n      11th_century\n      12th_century\n      13th_century\n      ...\n      Ziad_Jarrah\n      Zimbabwe\n      Zinc\n      Zinc_chloride\n      Zion_National_Park\n      Zionism\n      Zirconium\n      Zoroaster\n      Zuid-Gelders\n      Zulu\n    \n  \n  \n    \n      Áedán mac Gabráin\n      1.000000\n      0.003680\n      0.006182\n      0.047190\n      0.183846\n      0.004210\n      0.008076\n      0.012168\n      0.013427\n      0.011928\n      ...\n      0.004239\n      0.006244\n      0.001843\n      0.002172\n      0.003523\n      0.006111\n      0.001494\n      0.008908\n      0.001593\n      0.012721\n    \n    \n      Åland\n      0.003680\n      1.000000\n      0.008423\n      0.017851\n      0.004137\n      0.043866\n      0.007999\n      0.004936\n      0.004993\n      0.013233\n      ...\n      0.004768\n      0.023149\n      0.002752\n      0.002160\n      0.007089\n      0.016041\n      0.003315\n      0.005287\n      0.008188\n      0.012883\n    \n    \n      Édouard Manet\n      0.006182\n      0.008423\n      1.000000\n      0.009609\n      0.006520\n      0.014643\n      0.011617\n      0.008879\n      0.011491\n      0.011438\n      ...\n      0.010377\n      0.011835\n      0.004556\n      0.002780\n      0.011044\n      0.014414\n      0.004934\n      0.009887\n      0.005441\n      0.012838\n    \n    \n      Éire\n      0.047190\n      0.017851\n      0.009609\n      1.000000\n      0.049035\n      0.033312\n      0.012264\n      0.008489\n      0.033374\n      0.015251\n      ...\n      0.009314\n      0.038388\n      0.006863\n      0.005671\n      0.005486\n      0.031955\n      0.005751\n      0.012004\n      0.004430\n      0.019581\n    \n    \n      Óengus I of the Picts\n      0.183846\n      0.004137\n      0.006520\n      0.049035\n      1.000000\n      0.006741\n      0.012741\n      0.013508\n      0.020239\n      0.012106\n      ...\n      0.004763\n      0.008280\n      0.002700\n      0.002718\n      0.005656\n      0.008067\n      0.001780\n      0.009081\n      0.001923\n      0.015018\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Zionism\n      0.006111\n      0.016041\n      0.014414\n      0.031955\n      0.008067\n      0.017812\n      0.019210\n      0.021355\n      0.024060\n      0.023037\n      ...\n      0.012631\n      0.038386\n      0.007253\n      0.003734\n      0.028964\n      1.000000\n      0.006372\n      0.016985\n      0.004575\n      0.026654\n    \n    \n      Zirconium\n      0.001494\n      0.003315\n      0.004934\n      0.005751\n      0.001780\n      0.006597\n      0.007255\n      0.005258\n      0.004183\n      0.005319\n      ...\n      0.003589\n      0.010286\n      0.065117\n      0.038394\n      0.005937\n      0.006372\n      1.000000\n      0.005025\n      0.003256\n      0.004729\n    \n    \n      Zoroaster\n      0.008908\n      0.005287\n      0.009887\n      0.012004\n      0.009081\n      0.010104\n      0.021481\n      0.019697\n      0.017953\n      0.024732\n      ...\n      0.008748\n      0.014037\n      0.005008\n      0.003395\n      0.009401\n      0.016985\n      0.005025\n      1.000000\n      0.003077\n      0.011154\n    \n    \n      Zuid-Gelders\n      0.001593\n      0.008188\n      0.005441\n      0.004430\n      0.001923\n      0.005124\n      0.004908\n      0.004378\n      0.002217\n      0.004719\n      ...\n      0.003414\n      0.008758\n      0.002613\n      0.002273\n      0.004882\n      0.004575\n      0.003256\n      0.003077\n      1.000000\n      0.008353\n    \n    \n      Zulu\n      0.012721\n      0.012883\n      0.012838\n      0.019581\n      0.015018\n      0.011059\n      0.010697\n      0.010512\n      0.010046\n      0.015055\n      ...\n      0.007460\n      0.074077\n      0.003745\n      0.003207\n      0.009320\n      0.026654\n      0.004729\n      0.011154\n      0.008353\n      1.000000\n    \n  \n\n4604 rows × 4604 columns\n\n\n\n\n\nCode\n\n\n# save the Cosine Similarity matrix so we do not have to re-compute it every time\nnp.save('cosine_similarity.npz', np.array(cosine_sim))\n\n\n\n     \n\n\n\n\nCode\n\nimport matplotlib.pyplot as plt\n\n# let's take a look at the Cosine Similarity distribution of the most similar article to a given article\nmost_similar = []\nfor idx in range(len(cosine_sim)):\n    # index [-2] because index [-1] is always = 1 since it is a self similarity\n    most_similar.append(sorted(cosine_sim.iloc[idx])[-2])\n    \n# plot the similarities\npd.Series(most_similar).plot(kind='hist', edgecolor='k')\nplt.title(\"Cosine Similarity Distribution of Most Similar Articles\")\nplt.xlabel(\"Cosine Similarity\"); plt.ylabel(\"Absolute Counts\")\n\n# there are no glaring red flags, e.g., most Cosine Similarities close to 0 or 1\n\n\nText(0, 0.5, 'Absolute Counts')\n\n\n\n\n\n\n\nCode\n\n\n# show a few concrete examples and see if our metric makes empirical sense\ncosine_sim['Ukraine'].sort_values()\n\n# results look reasonable\n\n\n\n     \n\n\nPrimula              0.000621\nWren                 0.001130\nSaxicola             0.001339\nAngelica             0.001394\nNuthatch             0.001473\n                       ...   \nKiev                 0.340883\nHistory_of_Russia    0.356504\nPolish-Soviet_War    0.382866\nHero_of_Ukraine      0.403438\nUkraine              1.000000\nName: Ukraine, Length: 4604, dtype: float64\n\n\n\n\nCode\ncosine_sim['Zinc'].sort_values()\n# results also look reasonable\n\n\nPrimula          0.000303\nList_of_lakes    0.000396\nNuthatch         0.000659\nWren             0.000738\nWoodpecker       0.000761\n                   ...   \nMetal            0.158449\nCommon_cold      0.222766\nCadmium          0.247857\nZinc_chloride    0.511672\nZinc             1.000000\nName: Zinc, Length: 4604, dtype: float64\n\n\n\n\nCode\nimport seaborn as sns\n\n# \"Cygni\" are star systems/extrasolar planets and thus are expected to be similar\n# the time periods have some similarity\nsns.clustermap(cosine_sim.iloc[10:20, 10:20])\n\n\n<seaborn.matrix.ClusterGrid at 0x14dde5340>\n\n\n\n\n\n\n\nCode\n\n\n# \"Cancri\" are exoplanets and thus are expected to be similar\n# we again see similarity between time periods\nsns.clustermap(cosine_sim.iloc[50:60, 50:60])\n\n\n\n     \n\n\n<seaborn.matrix.ClusterGrid at 0x14ddd2490>\n\n\n\n\n\n\n\nCode\n\n\n# after confirming that the Cosine Similarity method is informative, we next show the function\n# we will use to extract similarity of articles in a navigation path\n\n# first show a hard-coded navigation path for illustration\nnavigation_path = '14th_century;Time;Light;Rainbow'\n\n\n\n     \n\n\n\n\nCode\n\ndef get_path_cosine_similarity(path: str, matrix: pd.DataFrame, method: str='sequential') -> list:\n    \"\"\"this function takes a navigation path string and returns an array of the Cosine Similarity.\n       Two methods are supported: 'sequential' (default) which measures the sequential similarity and \n       'target' which measures the similarity between the current article at the target article.\n       \n       Input:\n       path: navigation path string\n       matrix: pre-computed Cosine Similarity matrix\n       method: measurement method (defaults to 'sequential')\n       \n       Return:\n       list: list of similarity measures\n    \"\"\"\n    \n    # there is 1 article that is duplicated in the plaintext folder. Problematically, the duplicated\n    # article has different names and navigation paths can point to these different names, yet the \n    # article is the same. This one exception is handled here. In the implementation, we do not have to \n    # worry about this particular article pointing back to itself as it is not possible\n    \n    # navigation paths are separated by \";\"\n    path = path.split(';')\n    \n    # if the path only contains the starting article, return None\n    if len(path) == 1:\n        return None\n    \n    path_sims = []\n    \n    def get_correct_name(query: str, base_path: str='.data/plaintext_articles') -> str:\n        \"\"\"this nested function is called in case a path contains error characters.\n           Returns the fixed path so it can be used to query the Cosine Similarity matrix.\"\"\"\n        if '_' in query and '%' not in query:\n            return query\n        elif query in ['Podcasting', 'Color', 'Fencing', 'Anemia', 'Quito']:\n            return query\n        else:\n            with open(os.path.join(base_path, f'{query}.txt'), 'r') as f:\n                out = f.readlines()[2].replace('\\n', '')\n                return out\n    \n    # compute the similarity between sequential nodes\n    # output list length is N-1\n    if method == 'sequential':\n        for idx in range(0, len(path)-1, 1):\n            try:\n                path_sims.append(matrix.loc[path[idx]].loc[path[idx+1]])\n            except Exception:\n                # either path or both paths could be wrong - just \"fix\" both\n                # treat the 1 exception due to duplicated file\n                if get_correct_name(path[idx]) == 'Polish–Muscovite War (1605–1618)':\n                    sim = float(cosine_sim.loc['Polish–Muscovite War (1605–1618)'].drop_duplicates()[get_correct_name(path[idx+1])])\n                    path_sims.append(sim)\n                    \n                elif get_correct_name(path[idx+1]) == 'Polish–Muscovite War (1605–1618)':\n                    sim = cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polish–Muscovite War (1605–1618)']\n                    path_sims.append(sim)\n                    \n                else:\n                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[idx+1])])\n\n                    \n    # compute the similarity between the current node and the target node\n    # output list length is N\n    elif method == 'target':\n        for idx in range(0, len(path), 1):\n            try:\n                path_sims.append(matrix.loc[path[idx]].loc[path[-1]])\n            except Exception:\n                # either path or both paths could be wrong - just \"fix\" both\n                # treat the 1 exception due to duplicated file\n                if get_correct_name(path[idx]) == 'Polish–Muscovite War (1605–1618)':\n                    sim = cosine_sim.loc['Polish–Muscovite War (1605–1618)'].drop_duplicates()[get_correct_name(path[-1])]\n                    path_sims.append(sim)\n                    \n                elif get_correct_name(path[-1]) == 'Polish–Muscovite War (1605–1618)':\n                    sim = float(cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polish–Muscovite War (1605–1618)'])\n                    path_sims.append(sim)\n                    \n                else:\n                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[-1])])\n    \n    else:\n        raise ValueError('Unsupported similarity method: choose from \"sequential\" or \"target\".')\n        \n    return path_sims\n\n\n\n\nCode\nprint(navigation_path.split(';'))\n\n# show the \"sequential\" method\nsequential_sims = get_path_cosine_similarity(path=navigation_path,\n                                             matrix=cosine_sim,\n                                             method='sequential')\n# intepreted as:\n# sim(14th_century, Time) = 0.031\n# sim(Time, Light) = 0.147\n# sim(Light, Rainbow) = 0.158\n# the similarity list is always N-1 in length, N = # nodes\nsequential_sims\n\n\n['14th_century', 'Time', 'Light', 'Rainbow']\n\n\n[0.03148221080256591, 0.14746128033868, 0.15783064270493327]\n\n\n\n\nCode\n\n\n# show the \"target\" method\ntarget_sims = get_path_cosine_similarity(path=navigation_path,\n                                         matrix=cosine_sim,\n                                         method='target')\n# intepreted as:\n# sim(14th_century, Rainbow) = 0.008\n# sim(Time, Rainbow) = 0.035\n# sim(Light, Rainbow) = 0.158\n# sim(Rainbow, Rainbow) = 1 (the path was successful so similarity is 1)\n# the similarity list is now N in length, N = # nodes\n\n# note: paths where the last index != 1 means the path was unsuccessful\ntarget_sims\n\n\n\n     \n\n\n[0.00764736982703648,\n 0.03486926301697003,\n 0.15783064270493327,\n 1.0000000000000004]\n\n\n\n\nCode\n# calculate the \"sequential\" and \"target\" cosine similarities and augment the DataFrame with these metrics\nfinal_df['path_seq_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim))\nfinal_df['path_target_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim, method='target'))\n\nfinal_df.head(3)\n\n\n\n\n\n\n  \n    \n      \n      path\n      source_article\n      target_article\n      human_path_length\n      shortest_path_length\n      is_successful\n      path_seq_cosine_sim\n      path_target_cosine_sim\n    \n  \n  \n    \n      0\n      14th_century;Europe;Africa;Atlantic_slave_trad...\n      14th_century\n      African_slave_trade\n      4\n      3\n      1\n      [0.06712629693232745, 0.20733258691597292, 0.3...\n      [0.030770302481984573, 0.1043890593428143, 0.3...\n    \n    \n      1\n      14th_century;Europe;North_America;United_State...\n      14th_century\n      John_F._Kennedy\n      5\n      3\n      1\n      [0.06712629693232745, 0.17836530050620195, 0.3...\n      [0.012741833088878754, 0.02487036054931131, 0....\n    \n    \n      2\n      14th_century;China;Gunpowder;Fire\n      14th_century\n      Fire\n      3\n      3\n      1\n      [0.06407585068130972, 0.0719709239718701, 0.07...\n      [0.016703039404061093, 0.03765346608762515, 0....\n    \n  \n\n\n\n\n\n\nCode\n\n# next, write a function that returns whether the Cosine similarities in a navigation path are increasing/decreasing\ndef sims_to_bool_path(path: str):\n    \"\"\"this function takes a navgiation path as input and returns the path with same dimensions\n       containing Booleans denoting whether the Cosine similarity is increasing\"\"\"\n    bool_path = []\n    for idx in range(len(path)-1):\n        if path[idx+1] > path[idx]:\n            bool_path.append(True)\n        else:\n            bool_path.append(False)\n    \n    return bool_path\n        \n\n\n\n\nCode\n\n\nfinal_df['path_seq_boolean'] = final_df['path_seq_cosine_sim'].apply(sims_to_bool_path)\nfinal_df['path_target_boolean'] = final_df['path_target_cosine_sim'].apply(sims_to_bool_path)\n\nfinal_df.head(3)\n\n\n\n     \n\n\n\n\n\n\n  \n    \n      \n      path\n      source_article\n      target_article\n      human_path_length\n      shortest_path_length\n      is_successful\n      path_seq_cosine_sim\n      path_target_cosine_sim\n      path_seq_boolean\n      path_target_boolean\n    \n  \n  \n    \n      0\n      14th_century;Europe;Africa;Atlantic_slave_trad...\n      14th_century\n      African_slave_trade\n      4\n      3\n      1\n      [0.06712629693232745, 0.20733258691597292, 0.3...\n      [0.030770302481984573, 0.1043890593428143, 0.3...\n      [True, True, True]\n      [True, True, True, True]\n    \n    \n      1\n      14th_century;Europe;North_America;United_State...\n      14th_century\n      John_F._Kennedy\n      5\n      3\n      1\n      [0.06712629693232745, 0.17836530050620195, 0.3...\n      [0.012741833088878754, 0.02487036054931131, 0....\n      [True, True, False, False]\n      [True, True, True, True, True]\n    \n    \n      2\n      14th_century;China;Gunpowder;Fire\n      14th_century\n      Fire\n      3\n      3\n      1\n      [0.06407585068130972, 0.0719709239718701, 0.07...\n      [0.016703039404061093, 0.03765346608762515, 0....\n      [True, False]\n      [True, True, True]\n    \n  \n\n\n\n\n\n\nCode\n# this function filters paths that only contain strictly increasing Cosine similarities\ndef filter_strictly_increasing_sims(path_list: list):\n    return False if False in path_list else True\n\nfinal_df['seq_strictly_increasing'] = final_df['path_seq_boolean'].apply(filter_strictly_increasing_sims)\nfinal_df['target_strictly_increasing'] = final_df['path_target_boolean'].apply(filter_strictly_increasing_sims)\n\nfinal_df.head(3)\n\n\n\n\n\n\n  \n    \n      \n      path\n      source_article\n      target_article\n      human_path_length\n      shortest_path_length\n      is_successful\n      path_seq_cosine_sim\n      path_target_cosine_sim\n      path_seq_boolean\n      path_target_boolean\n      seq_strictly_increasing\n      target_strictly_increasing\n    \n  \n  \n    \n      0\n      14th_century;Europe;Africa;Atlantic_slave_trad...\n      14th_century\n      African_slave_trade\n      4\n      3\n      1\n      [0.06712629693232745, 0.20733258691597292, 0.3...\n      [0.030770302481984573, 0.1043890593428143, 0.3...\n      [True, True, True]\n      [True, True, True, True]\n      True\n      True\n    \n    \n      1\n      14th_century;Europe;North_America;United_State...\n      14th_century\n      John_F._Kennedy\n      5\n      3\n      1\n      [0.06712629693232745, 0.17836530050620195, 0.3...\n      [0.012741833088878754, 0.02487036054931131, 0....\n      [True, True, False, False]\n      [True, True, True, True, True]\n      False\n      True\n    \n    \n      2\n      14th_century;China;Gunpowder;Fire\n      14th_century\n      Fire\n      3\n      3\n      1\n      [0.06407585068130972, 0.0719709239718701, 0.07...\n      [0.016703039404061093, 0.03765346608762515, 0....\n      [True, False]\n      [True, True, True]\n      False\n      True\n    \n  \n\n\n\n\n\n\nCode\n\n\n# the matching algorithm requires some computation time\n# compute the matched DataFrames all at once and save them so they can be read into memory directly\n\n# there are 2 treatments\n#    1. strictly increasing *sequential* Cosine similarity\n#    2. strictly increasing *target* Cosine similarity\n\n# there are also 3 conditions we filter on\n#    1. human_path_length = 3\n#    2. human_path_length = 4\n#    3. human_path_length = 5\n\nimport networkx as nx\n\nfor treatment in ['seq_strictly_increasing', 'target_strictly_increasing']:\n    for length in [3,4,5]:\n    \n        treatment_df = final_df[final_df[treatment] == True]\n        control_df = final_df[final_df[treatment] == False]\n\n        G = nx.Graph()\n\n        for control_id, control_row in control_df.iterrows():\n            for treatment_id, treatment_row in treatment_df.iterrows():\n                # Adds an edge only for the same human path length\n                if (control_row['human_path_length'] == length and treatment_row['human_path_length'] == length):\n                    G.add_edge(control_id, treatment_id)\n\n        matching = nx.max_weight_matching(G)\n        matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n        balanced_df = final_df.iloc[matched]\n\n        # save balanced_df\n        balanced_df.to_csv(f'{treatment}_{length}.csv')\n\n\n\n     \n\n\nKeyboardInterrupt: \n\n\n\n\nCode\n\n\n# next, perform logistic regression on each treatment with fixed human path length\n\n# read all the matched DataFrames into memory\ndf_seq_3 = pd.read_csv('seq_strictly_increasing_3.csv')\ndf_seq_4 = pd.read_csv('seq_strictly_increasing_4.csv')\ndf_seq_5 = pd.read_csv('seq_strictly_increasing_5.csv')\n\ndf_target_3 = pd.read_csv('target_strictly_increasing_3.csv')\ndf_target_4 = pd.read_csv('target_strictly_increasing_4.csv')\ndf_target_5 = pd.read_csv('target_strictly_increasing_5.csv')\n\n# store all results in a dictionary\nresults = {}\n\nimport statsmodels.formula.api as smf\n\nfor df, experiment_name in zip([df_seq_3, df_seq_4, df_seq_5, df_target_3, df_target_4, df_target_5],\n                               ['Sequential 3', 'Sequential 4', 'Sequential 5', 'Target 3', 'Target 4', 'Target 5']):\n    \n    if 'Sequential' in experiment_name:\n        mod = smf.logit(formula='is_successful ~  seq_strictly_increasing', data=df)\n    else:\n        mod = smf.logit(formula='is_successful ~  target_strictly_increasing', data=df)\n        \n    res = mod.fit()\n    \n    # dictionary to store the coefficients and p-values of the current experiment\n    curr_analysis = {}\n    # store the intercept\n    curr_analysis['Intercept'] = res.params[0]\n    # store the intercept p-value\n    curr_analysis['Intercept p-value'] = res.pvalues[0]\n    # store the coefficient for **strictly increasing Cosine similarity**\n    curr_analysis['Coefficient'] = res.params[1]\n    # store the coefficient p-value\n    curr_analysis['Coefficient p-value'] = res.pvalues[1]\n    \n    results[experiment_name] = curr_analysis\n    \nresults_df = pd.DataFrame(results)\n\n\n\n     \nresults_df\n\n\nThe syntax in the above table is interpreted as :\nSequential means the strictly increasing Cosine similarity was measured for sequential articles\nTarget means the strictly increasing Cosine similarity was measured based on sequential article compared to the target article\nThe number indicates the human path length fixed. Previously, we also fixed that the shortest path length is 3.\nOur research question is:\nWhen given X number of choices in the same difficulty game, how did successful and unsuccessful players differ in their clicking behaviour?\nBased on the above table, the p-values for both the incercept and coefficient for logistic regression are significant for all experiments except 1 (Sequential 5). The interpretation for all experiment findings are similar. Concretely, consider the experiment Sequential 3. The interpretation is:\n*Given a shortest path length of 3 (fixed difficulty) and given a human path length of 3 (when the human makes 3 choices on article clicks), clicking articles with increasing Cosine similarity between sequential articles (as a proxy to measure semantic meaning), leads to more successful outcomes.*\nFor Target experiments have a similar interpretation. Concretely, consider the experiment Target 5:\n*Given a shortest path length of 3 (fixed difficulty) and given a human path length of 5 (when the human makes 5 choices on article clicks), clicking articles with increasing Cosine similarity between the current article and the target article (as a proxy to measure semantic meaning), leads to more successful outcomes.*\nThese findings are interesting and suggest that (potentially contrarily to common sense) clicking articles based on semantic meaning can lead to more successful outcomes in the Wikispeedia game (at least the current version we are investigating).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  }
]