[
  {
    "objectID": "datastory.html#wikispeedia-what-the-hell-is-this",
    "href": "datastory.html#wikispeedia-what-the-hell-is-this",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Wikispeedia: What the hell is this?",
    "text": "Wikispeedia: What the hell is this?\nSince the earliest records, humans have wandered across endless lands and walked thorugh mysterious paths, in an attempt to uncover the unexplored and find better conditions for their tribes. The rise of agriculture and the industrial era has, however, changed things, turning us into bare sedentary animalsâ€¦ until now!\n\nWikispeedia is an easy and fun game: You are given two Wikipedia articles, and starting from the first article, your goal is to reach the second one exclusively by following links in the articles you encounter, effectively letting you explore your own paths across the vast wikipedia, for free! Your game data is collected by The EPFL Data Science Lab, which helps us and them better understand some aspects of human behaviour ðŸ˜‰.\nUsing this data, we ask ourselves, are there patterns in the way players navigate the network, that make them more successful? That is, are there any optimal strategy that doesnâ€™t require extensive knowledge of the network?\nPlayers are faced with multiple stimulae during the playing session, and playersâ€™ decisions as well as their posterior success may be influenced by them. For example, users might navigate the network through semantic similarity optimization, or they might just click the links found in images or tables, etc.\nDo these particular features have any effect on playersâ€™ success? Letâ€™s see!\nWhatever"
  },
  {
    "objectID": "datastory.html#a-look-into-the-collected-data",
    "href": "datastory.html#a-look-into-the-collected-data",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "A look into the collected dataâ€¦",
    "text": "A look into the collected dataâ€¦"
  },
  {
    "objectID": "datastory.html#and-now-the-real-deal",
    "href": "datastory.html#and-now-the-real-deal",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "And now the real deal",
    "text": "And now the real deal\nWe wanna test wheather different strategies lead to success in the Wikispeedia game. With this goal in mind, paths can be characterized through local features of the clicked links and articles. The features we extract are:\n\nRelative position of each hyperlink within the current article.\nIs the hyperlink next to an image?\nIs the hyperlink inside a table?\nSimilarity between current and target article.\nHyperlink curvature along the path.\n\nOnce each path has been featurized we perform a matching analysis that lets us reduce the bias in the distributions of successful and unsuccessful populations. For this, we find pairs of subjects (paths) in the dataset, that have very similar features, but differ in the one we want to test (the treatment). The new distributions will then be much less biased and we will be able to compare the effect of the treatment variable.\n\nConfounding variables\nMany factors can affect the analysis, among them:\n\nthe difficulty of each task, and\nthe strategy adopted to solve it.\n\nTo address the first, we naively quantify difficulty of a task as the minimum distance between the source and the target, as determined with the Floyed-Warshall algorithm, while the second is addressed through path length, e.g.Â the total number of links clicked by each player in the given game.\nControlling by such variables allows a first approximation to controlling these confounding effects.\n\n\nTreatment\nWe will test the effect of clicking mostly links in image boxes.\nThe question is then: given an equally difficult task assigned, and having a fixed number of possible choices to perform, are players more successful if they adopt clicking mainly links in images as a strategy?\n\n\nCode\n\nsuccessful_df = successful_df[(successful_df['shortest_path_length'].apply(lambda x: x == 3))]\nsuccessful_df = successful_df[(successful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\nsuccessful_df.head(3)\n\n\n\n\n\n\nCode\n\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['shortest_path_length'].apply(lambda x: x == 3))]\nunsuccessful_df = unsuccessful_df[(unsuccessful_df['human_path_length'].apply(lambda x: 3 <= x <= 5))]\nunsuccessful_df.head(3)\n\n\nAt this point, we have filtered the successful and unsuccessful DataFrames by the following conditions:\nShortest path length = 3 --> we hypothesize that all paths have the same \"difficulty\" by enforcing this\nThe actual human path lengths are 3,4,5 --> the majority of human paths are within these lengths\nThe next step is to perform matching of datasets. To make this process possible, we first augment both successful and unsuccessful DataFrames with is_successful Boolean condition.\n\n\nCode\n\n\nsuccessful_df['is_successful'] = 1\nunsuccessful_df['is_successful'] = 0\n\n# merge the successful and unsuccessful DataFrames\nfinal_df = pd.concat([successful_df, unsuccessful_df])\nfinal_df.reset_index(drop=True, inplace=True)\n\nfinal_df.head(3)\n\n# save the DataFrame\nfinal_df.to_csv('final_df.csv')\n\n# next, we will calculate \"treatments\" involving semantic distance metrics for the DataFrame\n\n\n\n     \n\n\n2 . Document Similarity Similarity Between Wikipedia Articles: â€˜Bag-of-Wordsâ€™ Cosine Similarity Based on Word Frequencies\nWe are interested in measuring similarity between Wikipedia articles to investigate playersâ€™ strategies in Wikispeedia. The proposed similarity is Cosine Similarity measured on word frequencies given a pair of articles. Therefore, the assumption we make is that articles with similar occurrences of words are similar.\nIn order to mitigate similarity due to common English words such as â€œtheâ€, we apply a filter as follows:\nRemove all Stop words as defined in Scikit-learn\n\nhttps://scikit-learn.org/stable/modules/feature_extraction.html#nqy18\n\nRemove all white spaces and \\n characters\n\nApply a scaled version of word frequency count as implemented in TfidfTransformer in Scikit-learn:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\nThe cell blocks below show our data processing pipeline.\n\n\nCode\n\n\nimport os\n\nbase_path = '.data/plaintext_articles'\narticles = os.listdir(base_path)\n# sort the article names in alphabetical order. This is not strictly required\narticles = sorted(articles)\n\ndef remove_duplicates_and_fix_names(articles: list):\n    \"\"\"this function parses all the articles provided in the plaintext folder and stores the\n       names of all the articles after \"cleaning\" them (some articles containing accents have encoded names).\n       The raw text from the files are also stored in a list after removing white spaces and empty lines.\"\"\"\n    \n    def parse_text(article: str) -> str:\n        \"\"\"this helper function reads a raw text file and removes white spaces and empty lines.\"\"\"\n        with open(os.path.join(base_path, article)) as f:\n            # remove white spaces\n            raw_text = [line.rstrip() for line in f.readlines()]\n            # remove empty lines\n            raw_text = [line.strip() for line in raw_text if line != '']\n\n            return str(raw_text)\n    \n    # some article names have errors - fix these\n    article_names_cleaned, texts = [], []\n    \n    for article in articles:\n        # the \"%\" character indicates an encoded name\n        if \"%\" not in article:\n            # store the article name\n            article_names_cleaned.append(article)\n            # store the raw text from the article\n            texts.append(parse_text(article))\n            \n        else:\n            with open(os.path.join(base_path, article), 'r') as f:\n                # extract the correct name. It is always on the 3rd line of the article\n                correct_name = f.readlines()[2].replace('\\n', '')\n                # store the article name\n                article_names_cleaned.append(correct_name)\n                # store the raw text from the article\n                texts.append(parse_text(article))\n                    \n    \n    # remove \".txt\" from the article names\n    article_names_cleaned = [article.replace('.txt', '') for article in article_names_cleaned]\n    # remove list bracket from string casting of raw text\n    texts = [text.replace('[', '') for text in texts]\n    \n    return article_names_cleaned, texts\n\narticle_names_cleaned, texts = remove_duplicates_and_fix_names(articles)\n\n\n\n     \n\n\n\n\nCode\n\n# count word frequency using sklearn out-of-the-box functions\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport pandas as pd\nimport numpy as np\n\n# remove common stop words\ntfid_vectorizer = TfidfVectorizer(stop_words='english')\n# apply CountVectorizer and TfidfTransform sequentially\nmatrix = tfid_vectorizer.fit_transform(texts)\nmatrix = matrix.todense()\n\n# by default, \"TfidfVectorizer\" uses l2 norm and thus, to obtain the Cosine Similarity,\n# we simply perform a dot product of the matrix\ncosine_sim = pd.DataFrame(np.dot(np.array(matrix), np.array(matrix).T), columns=article_names_cleaned)\n# assign the index names also as the article names. This is used for easy querying pairs of articles\ncosine_sim.index = article_names_cleaned        \ncosine_sim\n\n\n\n\nCode\n\n\n# save the Cosine Similarity matrix so we do not have to re-compute it every time\nnp.save('cosine_similarity.npz', np.array(cosine_sim))\n\n\n\n     \n\n\n\n\nCode\n\nimport matplotlib.pyplot as plt\n\n# let's take a look at the Cosine Similarity distribution of the most similar article to a given article\nmost_similar = []\nfor idx in range(len(cosine_sim)):\n    # index [-2] because index [-1] is always = 1 since it is a self similarity\n    most_similar.append(sorted(cosine_sim.iloc[idx])[-2])\n    \n# plot the similarities\npd.Series(most_similar).plot(kind='hist', edgecolor='k')\nplt.title(\"Cosine Similarity Distribution of Most Similar Articles\")\nplt.xlabel(\"Cosine Similarity\"); plt.ylabel(\"Absolute Counts\")\n\n# there are no glaring red flags, e.g., most Cosine Similarities close to 0 or 1\n\n\n\n\nCode\n\n\n# show a few concrete examples and see if our metric makes empirical sense\ncosine_sim['Ukraine'].sort_values()\n\n# results look reasonable\n\n\n\n     \n\n\n\n\nCode\ncosine_sim['Zinc'].sort_values()\n# results also look reasonable\n\n\n\n\nCode\nimport seaborn as sns\n\n# \"Cygni\" are star systems/extrasolar planets and thus are expected to be similar\n# the time periods have some similarity\nsns.clustermap(cosine_sim.iloc[10:20, 10:20])\n\n\n\n\nCode\n\n\n# \"Cancri\" are exoplanets and thus are expected to be similar\n# we again see similarity between time periods\nsns.clustermap(cosine_sim.iloc[50:60, 50:60])\n\n\n\n     \n\n\n\n\nCode\n\n\n# after confirming that the Cosine Similarity method is informative, we next show the function\n# we will use to extract similarity of articles in a navigation path\n\n# first show a hard-coded navigation path for illustration\nnavigation_path = '14th_century;Time;Light;Rainbow'\n\n\n\n     \n\n\n\n\nCode\n\ndef get_path_cosine_similarity(path: str, matrix: pd.DataFrame, method: str='sequential') -> list:\n    \"\"\"this function takes a navigation path string and returns an array of the Cosine Similarity.\n       Two methods are supported: 'sequential' (default) which measures the sequential similarity and \n       'target' which measures the similarity between the current article at the target article.\n       \n       Input:\n       path: navigation path string\n       matrix: pre-computed Cosine Similarity matrix\n       method: measurement method (defaults to 'sequential')\n       \n       Return:\n       list: list of similarity measures\n    \"\"\"\n    \n    # there is 1 article that is duplicated in the plaintext folder. Problematically, the duplicated\n    # article has different names and navigation paths can point to these different names, yet the \n    # article is the same. This one exception is handled here. In the implementation, we do not have to \n    # worry about this particular article pointing back to itself as it is not possible\n    \n    # navigation paths are separated by \";\"\n    path = path.split(';')\n    \n    # if the path only contains the starting article, return None\n    if len(path) == 1:\n        return None\n    \n    path_sims = []\n    \n    def get_correct_name(query: str, base_path: str='.data/plaintext_articles') -> str:\n        \"\"\"this nested function is called in case a path contains error characters.\n           Returns the fixed path so it can be used to query the Cosine Similarity matrix.\"\"\"\n        if '_' in query and '%' not in query:\n            return query\n        elif query in ['Podcasting', 'Color', 'Fencing', 'Anemia', 'Quito']:\n            return query\n        else:\n            with open(os.path.join(base_path, f'{query}.txt'), 'r') as f:\n                out = f.readlines()[2].replace('\\n', '')\n                return out\n    \n    # compute the similarity between sequential nodes\n    # output list length is N-1\n    if method == 'sequential':\n        for idx in range(0, len(path)-1, 1):\n            try:\n                path_sims.append(matrix.loc[path[idx]].loc[path[idx+1]])\n            except Exception:\n                # either path or both paths could be wrong - just \"fix\" both\n                # treat the 1 exception due to duplicated file\n                if get_correct_name(path[idx]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = float(cosine_sim.loc['Polishâ€“Muscovite War (1605â€“1618)'].drop_duplicates()[get_correct_name(path[idx+1])])\n                    path_sims.append(sim)\n                    \n                elif get_correct_name(path[idx+1]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polishâ€“Muscovite War (1605â€“1618)']\n                    path_sims.append(sim)\n                    \n                else:\n                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[idx+1])])\n\n                    \n    # compute the similarity between the current node and the target node\n    # output list length is N\n    elif method == 'target':\n        for idx in range(0, len(path), 1):\n            try:\n                path_sims.append(matrix.loc[path[idx]].loc[path[-1]])\n            except Exception:\n                # either path or both paths could be wrong - just \"fix\" both\n                # treat the 1 exception due to duplicated file\n                if get_correct_name(path[idx]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = cosine_sim.loc['Polishâ€“Muscovite War (1605â€“1618)'].drop_duplicates()[get_correct_name(path[-1])]\n                    path_sims.append(sim)\n                    \n                elif get_correct_name(path[-1]) == 'Polishâ€“Muscovite War (1605â€“1618)':\n                    sim = float(cosine_sim.loc[get_correct_name(path[idx])].drop_duplicates()['Polishâ€“Muscovite War (1605â€“1618)'])\n                    path_sims.append(sim)\n                    \n                else:\n                    path_sims.append(matrix.loc[get_correct_name(path[idx])].loc[get_correct_name(path[-1])])\n    \n    else:\n        raise ValueError('Unsupported similarity method: choose from \"sequential\" or \"target\".')\n        \n    return path_sims\n\n\n\n\nCode\nprint(navigation_path.split(';'))\n\n# show the \"sequential\" method\nsequential_sims = get_path_cosine_similarity(path=navigation_path,\n                                             matrix=cosine_sim,\n                                             method='sequential')\n# intepreted as:\n# sim(14th_century, Time) = 0.031\n# sim(Time, Light) = 0.147\n# sim(Light, Rainbow) = 0.158\n# the similarity list is always N-1 in length, N = # nodes\nsequential_sims\n\n\n\n\nCode\n\n\n# show the \"target\" method\ntarget_sims = get_path_cosine_similarity(path=navigation_path,\n                                         matrix=cosine_sim,\n                                         method='target')\n# intepreted as:\n# sim(14th_century, Rainbow) = 0.008\n# sim(Time, Rainbow) = 0.035\n# sim(Light, Rainbow) = 0.158\n# sim(Rainbow, Rainbow) = 1 (the path was successful so similarity is 1)\n# the similarity list is now N in length, N = # nodes\n\n# note: paths where the last index != 1 means the path was unsuccessful\ntarget_sims\n\n\n\n     \n\n\n\n\nCode\n# calculate the \"sequential\" and \"target\" cosine similarities and augment the DataFrame with these metrics\nfinal_df['path_seq_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim))\nfinal_df['path_target_cosine_sim'] = final_df['path'].apply(lambda x: get_path_cosine_similarity(x, matrix=cosine_sim, method='target'))\n\nfinal_df.head(3)\n\n\n\n\nCode\n\n# next, write a function that returns whether the Cosine similarities in a navigation path are increasing/decreasing\ndef sims_to_bool_path(path: str):\n    \"\"\"this function takes a navgiation path as input and returns the path with same dimensions\n       containing Booleans denoting whether the Cosine similarity is increasing\"\"\"\n    bool_path = []\n    for idx in range(len(path)-1):\n        if path[idx+1] > path[idx]:\n            bool_path.append(True)\n        else:\n            bool_path.append(False)\n    \n    return bool_path\n        \n\n\n\n\nCode\n\n\nfinal_df['path_seq_boolean'] = final_df['path_seq_cosine_sim'].apply(sims_to_bool_path)\nfinal_df['path_target_boolean'] = final_df['path_target_cosine_sim'].apply(sims_to_bool_path)\n\nfinal_df.head(3)\n\n\n\n     \n\n\n\n\nCode\n# this function filters paths that only contain strictly increasing Cosine similarities\ndef filter_strictly_increasing_sims(path_list: list):\n    return False if False in path_list else True\n\nfinal_df['seq_strictly_increasing'] = final_df['path_seq_boolean'].apply(filter_strictly_increasing_sims)\nfinal_df['target_strictly_increasing'] = final_df['path_target_boolean'].apply(filter_strictly_increasing_sims)\n\nfinal_df.head(3)\n\n\n\n\nCode\n\n\n# the matching algorithm requires some computation time\n# compute the matched DataFrames all at once and save them so they can be read into memory directly\n\n# there are 2 treatments\n#    1. strictly increasing *sequential* Cosine similarity\n#    2. strictly increasing *target* Cosine similarity\n\n# there are also 3 conditions we filter on\n#    1. human_path_length = 3\n#    2. human_path_length = 4\n#    3. human_path_length = 5\n\nimport networkx as nx\n\nfor treatment in ['seq_strictly_increasing', 'target_strictly_increasing']:\n    for length in [3,4,5]:\n    \n        treatment_df = final_df[final_df[treatment] == True]\n        control_df = final_df[final_df[treatment] == False]\n\n        G = nx.Graph()\n\n        for control_id, control_row in control_df.iterrows():\n            for treatment_id, treatment_row in treatment_df.iterrows():\n                # Adds an edge only for the same human path length\n                if (control_row['human_path_length'] == length and treatment_row['human_path_length'] == length):\n                    G.add_edge(control_id, treatment_id)\n\n        matching = nx.max_weight_matching(G)\n        matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n        balanced_df = final_df.iloc[matched]\n\n        # save balanced_df\n        balanced_df.to_csv(f'{treatment}_{length}.csv')\n\n\n\n     \n\n\n\n\nCode\n\n\n# next, perform logistic regression on each treatment with fixed human path length\n\n# read all the matched DataFrames into memory\ndf_seq_3 = pd.read_csv('seq_strictly_increasing_3.csv')\ndf_seq_4 = pd.read_csv('seq_strictly_increasing_4.csv')\ndf_seq_5 = pd.read_csv('seq_strictly_increasing_5.csv')\n\ndf_target_3 = pd.read_csv('target_strictly_increasing_3.csv')\ndf_target_4 = pd.read_csv('target_strictly_increasing_4.csv')\ndf_target_5 = pd.read_csv('target_strictly_increasing_5.csv')\n\n# store all results in a dictionary\nresults = {}\n\nimport statsmodels.formula.api as smf\n\nfor df, experiment_name in zip([df_seq_3, df_seq_4, df_seq_5, df_target_3, df_target_4, df_target_5],\n                               ['Sequential 3', 'Sequential 4', 'Sequential 5', 'Target 3', 'Target 4', 'Target 5']):\n    \n    if 'Sequential' in experiment_name:\n        mod = smf.logit(formula='is_successful ~  seq_strictly_increasing', data=df)\n    else:\n        mod = smf.logit(formula='is_successful ~  target_strictly_increasing', data=df)\n        \n    res = mod.fit()\n    \n    # dictionary to store the coefficients and p-values of the current experiment\n    curr_analysis = {}\n    # store the intercept\n    curr_analysis['Intercept'] = res.params[0]\n    # store the intercept p-value\n    curr_analysis['Intercept p-value'] = res.pvalues[0]\n    # store the coefficient for **strictly increasing Cosine similarity**\n    curr_analysis['Coefficient'] = res.params[1]\n    # store the coefficient p-value\n    curr_analysis['Coefficient p-value'] = res.pvalues[1]\n    \n    results[experiment_name] = curr_analysis\n    \nresults_df = pd.DataFrame(results)\n\n\n\n     \nresults_df\n\n\nThe syntax in the above table is interpreted as :\nSequential means the strictly increasing Cosine similarity was measured for sequential articles\nTarget means the strictly increasing Cosine similarity was measured based on sequential article compared to the target article\nThe number indicates the human path length fixed. Previously, we also fixed that the shortest path length is 3.\nOur research question is:\nWhen given X number of choices in the same difficulty game, how did successful and unsuccessful players differ in their clicking behaviour?\nBased on the above table, the p-values for both the incercept and coefficient for logistic regression are significant for all experiments except 1 (Sequential 5). The interpretation for all experiment findings are similar. Concretely, consider the experiment Sequential 3. The interpretation is:\n*Given a shortest path length of 3 (fixed difficulty) and given a human path length of 3 (when the human makes 3 choices on article clicks), clicking articles with increasing Cosine similarity between sequential articles (as a proxy to measure semantic meaning), leads to more successful outcomes.*\nFor Target experiments have a similar interpretation. Concretely, consider the experiment Target 5:\n*Given a shortest path length of 3 (fixed difficulty) and given a human path length of 5 (when the human makes 5 choices on article clicks), clicking articles with increasing Cosine similarity between the current article and the target article (as a proxy to measure semantic meaning), leads to more successful outcomes.*\nThese findings are interesting and suggest that (potentially contrarily to common sense) clicking articles based on semantic meaning can lead to more successful outcomes in the Wikispeedia game (at least the current version we are investigating).\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\nFigureÂ 1: ?(caption)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The ADAlyzers is a non-profit founded by 3 amazing students at EPFL."
  },
  {
    "objectID": "datastory.html#jeffs-plots-and-visualization",
    "href": "datastory.html#jeffs-plots-and-visualization",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "JEFFâ€™s PLOTS and VISUALIZATION",
    "text": "JEFFâ€™s PLOTS and VISUALIZATION\n\n\nCode\nfrom IPython.display import IFrame\nIFrame(\"images/path.pdf\", width=600, height=300)\n\n\n\n        \n        \n\n\n\n\nCode\n#from wand.image import Image as WImage\n#img = WImage(filename='images/path.pdf')"
  },
  {
    "objectID": "datastory.html#jeffs-plots",
    "href": "datastory.html#jeffs-plots",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "JEFFâ€™s PLOTS",
    "text": "JEFFâ€™s PLOTS\n\n\nCode\nfrom IPython.display import IFrame\nIFrame(\"images/path.pdf\", width=600, height=300)\n\n\n\n        \n        \n\n\n\n\nCode\n#from wand.image import Image as WImage\n#img = WImage(filename='images/path.pdf')"
  },
  {
    "objectID": "datastory.html#wikispeedia-what-does-x-have-to-do-with-y-you-better-find-out",
    "href": "datastory.html#wikispeedia-what-does-x-have-to-do-with-y-you-better-find-out",
    "title": "Wikispeedia: An exploration into human strategy through path navigation.",
    "section": "Wikispeedia: What does X have to do with Y? You better find out!",
    "text": "Wikispeedia: What does X have to do with Y? You better find out!\nSince the earliest records, humans have wandered across endless lands and walked thorugh mysterious paths, in an attempt to uncover the unexplored and find better conditions for their tribes. The rise of agriculture and the industrial era has, however, changed things, turning us into bare sedentary animalsâ€¦ until now!\n\nWikispeedia is an easy and fun game: You are given two Wikipedia articles, and starting from the first article, your goal is to reach the second one exclusively by following links in the articles you encounter, effectively letting you explore your own paths across the vast wikipedia, for free! Your game data is collected by The EPFL Data Science Lab, which helps us and them better understand some aspects of human behaviour ðŸ˜‰.\nUsing this data, we ask ourselves, are there patterns in the way players navigate the network, that make them more successful? That is, are there any optimal strategy that doesnâ€™t require extensive knowledge of the network?\nPlayers are faced with multiple stimulae during the playing session, and playersâ€™ decisions as well as their posterior success may be influenced by them. For example, users might navigate the network through semantic similarity optimization, or they might just click the links found in images or tables, etc.\nDo these particular features have any effect on playersâ€™ success? Letâ€™s find out!"
  }
]